{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/AD/kachiem/miniconda3/envs/tf-conda/bin/python3\n",
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob as glob\n",
    "from os.path import join, basename, dirname, exists\n",
    "import sys\n",
    "print(sys.executable)  #print kernel path\n",
    "#print(sys.path)\n",
    "\n",
    "# tensorflow imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "print(tf.__version__)\n",
    "\n",
    "# custom imports\n",
    "import librispect as lspct\n",
    "from librispect.features import predict\n",
    "from librispect.utils import split_validation\n",
    "\n",
    "# cpc import\n",
    "sys.path.insert(1, '../')\n",
    "from data_utils import SortedNumberGenerator\n",
    "sys.path.insert(1, 'notebooks/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attempting to connect srihita's memmap spectrogram function to train_model  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Instead of images and labels, I have spectrogram segments x and y.\n",
    "So x is input and y would be output (aka labels). \n",
    "You train on bunch of (x - power,y - number of samples), let's call it train_x and train_y.\n",
    "Then when you input test_x, you should get test_y (or vis_x and vis_y in this case)\n",
    "'''\n",
    "\n",
    "# spect_height = number of freq. bins\n",
    "# y always has one time bin\n",
    "# n_lags is number of time bins of x\n",
    "# shape of x = (n_lags,spect_height) x number of samples\n",
    "# shape of y = (1, spect_height) x number of samples\n",
    "\n",
    "n_lags = 2\n",
    "spect_height = 32\n",
    "NIN = n_lags*spect_height\n",
    "\n",
    "# args for the following class, MemmapHandler\n",
    "datafolder = '/home/AD/kachiem/memmap/memmap_dataset_stimulus/'\n",
    "visfolder = '/home/AD/kachiem/memmap/memmap_dataset_stimulus_vis/'\n",
    "n_lags = 2\n",
    "spect_height = 32\n",
    "#subset = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to array\n",
    "def memmap_to_arr(memmap, shape):\n",
    "    # memmap: input np.memmap()\n",
    "    # shape: (samples, channels, features (freq))\n",
    "    \n",
    "    memmap = np.asarray(memmap.T[:].tolist()).reshape(shape)\n",
    "    lst = [np.asarray(memmap[i]) for i in range(0,len(memmap))]\n",
    "    arr = np.array(lst)\n",
    "    print(arr.shape)\n",
    "    \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149962, 2, 32)\n",
      "(149962, 2, 32)\n"
     ]
    }
   ],
   "source": [
    "x_data = memmap_to_arr(xdata, (xdata.shape[1], 2, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4798784,), (64, 149962), (32, 149962))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading train data\n",
    "why = np.lib.format.open_memmap('y.npy', dtype='float32', mode='r')\n",
    "xdata = np.lib.format.open_memmap('x_data.npy', dtype='float32', mode='r', shape=(n_lags*spect_height, num_data_samples))\n",
    "ydata = np.lib.format.open_memmap('y_data.npy', dtype='float32', mode='r', shape=(spect_height, num_data_samples))\n",
    "\n",
    "num_data_samples = int(len(why) / spect_height)\n",
    "\n",
    "why.shape, xdata.shape, ydata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149962, 2, 32)\n",
      "(149962, 1, 32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4798784,)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = why[:]\n",
    "x_data = memmap_to_arr(xdata, (xdata.shape[1], 2, 32))\n",
    "y_data = memmap_to_arr(ydata, (ydata.shape[1], 1, 32))\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16653, 2, 32), (16653, 1, 32))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subsetting to match vis data\n",
    "x_data = x_data[:16653, :, :]\n",
    "y_data = y_data[:16653, :, :]\n",
    "\n",
    "x_data.shape, y_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 16653), (64, 16653), (32, 16653))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading test data\n",
    "why_vis = np.lib.format.open_memmap('y_vis.npy', dtype='float32', mode='r')\n",
    "xdata_vis = np.lib.format.open_memmap('x_data_vis.npy', dtype='float32', mode='r', shape=(n_lags*spect_height, num_data_samples_vis))\n",
    "ydata_vis = np.lib.format.open_memmap('y_data_vis.npy', dtype='float32', mode='r', shape=(spect_height, num_data_samples_vis))\n",
    "\n",
    "num_data_samples_vis = int(len(why_vis) / spect_height)\n",
    "\n",
    "why_vis.shape, xdata_vis.shape, ydata_vis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16653, 2, 32)\n",
      "(16653, 1, 32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32, 16653)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_vis = why_vis[:]\n",
    "x_data_vis = memmap_to_arr(xdata_vis, (xdata_vis.shape[1], 2, 32))\n",
    "y_data_vis = memmap_to_arr(ydata_vis, (ydata_vis.shape[1], 1, 32))\n",
    "\n",
    "data_vis.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data (arrays) into iterator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Shuffled_memmap_subset(object):\n",
    "    \n",
    "    ''' For creating train/valid/test shuffled memmap dataset '''\n",
    "    \n",
    "    def __init__(self, data, x_data, y_data, subset, n_lags, spect_height, vis_ratio=0, valid_ratio=.06):\n",
    "        \n",
    "        # Set params\n",
    "        self.data = data\n",
    "        self.x_data = x_data\n",
    "        self.y_data = y_data\n",
    "        self.subset = subset\n",
    "        self.n_lags = n_lags\n",
    "        self.spect_height = spect_height\n",
    "        self.num_data_samples = int(len(data) / spect_height)\n",
    "        #self.num_data_samples\n",
    "\n",
    "        '''\n",
    "        # Initialize memmap handler\n",
    "        self.memmap_handler = MemmapHandler(self.folder, self.n_lags, self.spect_height, self.subset)\n",
    "\n",
    "        # Initialize data\n",
    "        data_loc = self.memmap_handler.read_file_loc(self.folder)\n",
    "        data = self.memmap_handler.init_data(self.folder, self.saved_data, x_location)\n",
    "        self.num_data_samples = self.memmap_handler.get_n_samples('train')\n",
    "        self.dataset = data.memmap_copy(data, data.shape[0], data.shape[1][:1000])\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        # get dataset+batching indicies\n",
    "        #self.num_vis_samples = int(self.num_data_samples*vis_ratio)\n",
    "        self.num_valid_samples = int(self.num_data_samples*valid_ratio)\n",
    "        #vis_start = np.random.randint(self.num_data_samples - self.num_vis_samples)\n",
    "       \n",
    "        #self.vis_idxs = np.arange(vis_start, vis_start + self.num_vis_samples)\n",
    "        #self.data_idxs = np.delete(np.arange(self.num_data_samples), self.vis_idxs)\n",
    "        self.data_idxs = np.arange(self.num_data_samples)\n",
    "        #self.num_data_samples -= self.num_vis_samples\n",
    "        self.n_batches = self.get_n_batches()\n",
    "       \n",
    "        np.random.shuffle(self.data_idxs)\n",
    "        self.valid_idxs, self.data_idxs = np.split(self.data_idxs, [self.num_valid_samples])\n",
    "        self.num_data_samples -= self.num_valid_samples\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_batches\n",
    "    \n",
    "    def get_n_batches(self, batch_size=64):\n",
    "        n_batches = 0\n",
    "        for batch_idx in range(0, self.num_data_samples, batch_size):\n",
    "            n_batches +=1\n",
    "        return n_batches\n",
    "            \n",
    "    def get_num_samples(self, subset):\n",
    "        ''' \n",
    "        Return number of samples wrt subset. \n",
    "            subset: a string - 'train', 'valid', or 'test'\n",
    "        '''\n",
    "        if subset == 'train':\n",
    "            return self.num_data_samples\n",
    "        elif subset == 'valid':\n",
    "            return self.num_valid_samples\n",
    "        elif subset == 'test':\n",
    "            return self.num_data_samples_vis\n",
    "    \n",
    "    def get_data(self, idx):\n",
    "        return self.x_data[:, :idx].T, self.y_data[idx].T\n",
    "   \n",
    "    def data_iterator(self, batch_size=64):\n",
    "        np.random.shuffle(self.data_idxs)\n",
    "        for batch_idx in range(0, self.num_data_samples, batch_size):\n",
    "            shuff_idx = self.data_idxs[batch_idx:batch_idx+batch_size]\n",
    "            \n",
    "            yield self.get_data(shuff_idx)\n",
    "   \n",
    "    def valid_set(self):\n",
    "        return self.get_data(self.valid_idxs)\n",
    "   \n",
    "    def valid_iterator(self, batch_size=64):\n",
    "        for batch_idx in range(0, self.num_valid_samples, batch_size):\n",
    "            valid_idx = self.valid_idxs[batch_idx:batch_idx+batch_size]\n",
    "            yield self.get_data(valid_idx)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test iterator object\n",
    "train_data = Shuffled_memmap_subset(\n",
    "    data=data,\n",
    "    x_data=x_data,\n",
    "    y_data=y_data,\n",
    "    subset='train', \n",
    "    n_lags=2,\n",
    "    spect_height=32\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_encoder(x, code_size):\n",
    "\n",
    "    ''' Define the network mapping images to embeddings '''\n",
    "    print(x.shape)\n",
    "    x = keras.layers.Conv1D(filters=2, kernel_size=2, strides=1, data_format=\"channels_last\", activation='relu', input_shape=(32, 2))(x)\n",
    "    print(\"conv1d\", x.shape)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.LeakyReLU()(x)\n",
    "    x = keras.layers.Conv1D(filters=2, kernel_size=2, strides=1, activation='relu')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.LeakyReLU()(x)\n",
    "    x = keras.layers.Conv1D(filters=4, kernel_size=2, strides=2, activation='relu')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.LeakyReLU()(x)\n",
    "    x = keras.layers.Conv1D(filters=4, kernel_size=2, strides=2, activation='relu')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.LeakyReLU()(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Dense(units=512, activation='relu')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.LeakyReLU()(x)\n",
    "    x = keras.layers.Dense(units=code_size, activation='relu', name='encoder_embedding')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def network_autoregressive(x):\n",
    "\n",
    "    ''' Define the network that integrates information along the sequence '''\n",
    "\n",
    "    # x = keras.layers.GRU(units=256, return_sequences=True)(x)\n",
    "    # x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.GRU(units=256, return_sequences=False, name='ar_context')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def network_prediction(context, code_size, predict_terms):\n",
    "\n",
    "    ''' Define the network mapping context to multiple embeddings '''\n",
    "\n",
    "    outputs = []\n",
    "    for i in range(predict_terms):\n",
    "        outputs.append(keras.layers.Dense(units=code_size, activation=\"linear\", name='z_t_{i}'.format(i=i))(context))\n",
    "\n",
    "    if len(outputs) == 1:\n",
    "        output = keras.layers.Lambda(lambda x: K.expand_dims(x, axis=1))(outputs[0])\n",
    "    else:\n",
    "        output = keras.layers.Lambda(lambda x: K.stack(x, axis=1))(outputs)\n",
    "\n",
    "    return output\n",
    "\n",
    "class CPCLayer(keras.layers.Layer):\n",
    "\n",
    "    ''' Computes dot product between true and predicted embedding vectors '''\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CPCLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        # Compute dot product among vectors\n",
    "        preds, y_encoded = inputs\n",
    "        dot_product = K.mean(y_encoded * preds, axis=-1)\n",
    "        dot_product = K.mean(dot_product, axis=-1, keepdims=True)  # average along the temporal dimension\n",
    "\n",
    "        # Keras loss functions take probabilities\n",
    "        dot_product_probs = K.sigmoid(dot_product)\n",
    "\n",
    "        return dot_product_probs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        #return (input_shape[0][0],1)\n",
    "        return (input_shape[0], 1)\n",
    "\n",
    "def network_cpc(spect_height, n_lags, terms, predict_terms, code_size, input_shape, learning_rate):\n",
    "\n",
    "    ''' Define the CPC network combining encoder and autoregressive model '''\n",
    "\n",
    "    # Set learning phase (https://stackoverflow.com/questions/42969779/keras-error-you-must-feed-a-value-for-placeholder-tensor-bidirectional-1-keras)\n",
    "    K.set_learning_phase(1)\n",
    "\n",
    "    # Define encoder model\n",
    "    encoder_input = keras.layers.Input(shape=(spect_height, n_lags))\n",
    "    #print(encoder_input.shape)\n",
    "    encoder_output = network_encoder(encoder_input, code_size)\n",
    "    #print(encoder_output.shape)\n",
    "    encoder_model = keras.models.Model(encoder_input, encoder_output, name='encoder')\n",
    "    encoder_model.summary()\n",
    "\n",
    "    # Define rest of model\n",
    "    x_input = keras.layers.Input((terms, spect_height, n_lags))\n",
    "    x_encoded = keras.layers.TimeDistributed(encoder_model)(x_input)\n",
    "    context = network_autoregressive(x_encoded)\n",
    "    preds = network_prediction(context, code_size, predict_terms)\n",
    "\n",
    "    y_input = keras.layers.Input((predict_terms, spect_height, n_lags))\n",
    "    y_encoded = keras.layers.TimeDistributed(encoder_model)(y_input)\n",
    "\n",
    "    # Loss\n",
    "    dot_product_probs = CPCLayer()([preds, y_encoded])\n",
    "\n",
    "    # Model\n",
    "    cpc_model = keras.models.Model(inputs=[x_input, y_input], outputs=dot_product_probs)\n",
    "\n",
    "    # Compile model\n",
    "    cpc_model.compile(\n",
    "        optimizer=keras.optimizers.Adam(lr=learning_rate),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['binary_accuracy']\n",
    "    )\n",
    "    cpc_model.summary()\n",
    "\n",
    "    return cpc_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memmap ver\n",
    "def train_model(data, x_data, y_data, data_vis, x_data_vis, y_data_vis, n_lags, spect_height, epochs, batch_size, output_dir, code_size, lr=2e-4, terms=3, predict_terms=3, color=True):\n",
    "\n",
    "    # Prepare data\n",
    "    # clean up generator more to work by subset\n",
    "    train_data = Shuffled_memmap_subset(\n",
    "        data=data,\n",
    "        x_data=x_data,\n",
    "        y_data=y_data,\n",
    "        subset='train', \n",
    "        n_lags=n_lags,\n",
    "        spect_height=spect_height\n",
    "        )\n",
    "\n",
    "    test_data = Shuffled_memmap_subset(\n",
    "        data=data_vis,\n",
    "        x_data=x_data_vis,\n",
    "        y_data=y_data_vis,\n",
    "        subset='test', \n",
    "        n_lags=n_lags,\n",
    "        spect_height=spect_height\n",
    "        )\n",
    "    \n",
    "    # Prepares the model\n",
    "    model = network_cpc(spect_height=spect_height, n_lags=n_lags, terms=terms, predict_terms=predict_terms,\n",
    "                        code_size=code_size, input_shape=(spect_height, n_lags), learning_rate=lr)\n",
    "\n",
    "    # Callbacks\n",
    "    history = keras.callbacks.History()\n",
    "    reduce_LR_Plateau = keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', \n",
    "        factor=1/3, \n",
    "        patience=2, \n",
    "        min_lr=1e-4)\n",
    "    #callbacks = [history, reduce_LR_Plateau]\n",
    "    callbacks = [history]\n",
    "    \n",
    "    # Trains the model\n",
    "    modeled = model.fit_generator(\n",
    "        generator=train_data.data_iterator(),\n",
    "        steps_per_epoch=len(train_data),\n",
    "        validation_data=test_data.data_iterator(),\n",
    "        validation_steps=len(test_data),\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    # Saves the model\n",
    "    # Remember to add custom_objects={'CPCLayer': CPCLayer} to load_model when loading from disk\n",
    "    model.save(join(output_dir, 'mem_cpc.h5'))\n",
    "\n",
    "    # Saves the encoder alone\n",
    "    encoder = model.layers[1].layer\n",
    "    encoder.save(join(output_dir, 'mem_encoder.h5'))\n",
    "    \n",
    "    # plotting loss\n",
    "    train_loss = modeled.history['loss']\n",
    "    val_loss = modeled.history['val_loss']\n",
    "    epoch_count = range(1, epochs+1)    # 10 epochs\n",
    "\n",
    "    plt.plot(epoch_count, train_loss, 'r--')\n",
    "    plt.plot(epoch_count, val_loss, 'o-')\n",
    "    plt.legend(['Training Loss', 'Validation Loss'])\n",
    "    plt.title('')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    t = time.localtime()\n",
    "    timestamp = time.strftime('%b-%d-%Y_%H%M', t)\n",
    "    plt.savefig(output_dir + \"img/\" + timestamp + \"_mem.png\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 32, 2)\n",
      "conv1d (?, 31, 2)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 32, 2)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 31, 2)             10        \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_10 (B (None, 31, 2)             8         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 31, 2)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 30, 2)             10        \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_11 (B (None, 30, 2)             8         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 30, 2)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 15, 4)             20        \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_12 (B (None, 15, 4)             16        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 15, 4)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 7, 4)              36        \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_13 (B (None, 7, 4)              16        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 7, 4)              0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 28)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               14848     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_14 (B (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "encoder_embedding (Dense)    (None, 512)               262656    \n",
      "=================================================================\n",
      "Total params: 279,676\n",
      "Trainable params: 278,628\n",
      "Non-trainable params: 1,048\n",
      "_________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, 3, 32, 2)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, 3, 512)       279676      input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "ar_context (GRU)                (None, 256)          590592      time_distributed_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "z_t_0 (Dense)                   (None, 512)          131584      ar_context[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "z_t_1 (Dense)                   (None, 512)          131584      ar_context[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "z_t_2 (Dense)                   (None, 512)          131584      ar_context[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, 3, 32, 2)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 3, 512)       0           z_t_0[0][0]                      \n",
      "                                                                 z_t_1[0][0]                      \n",
      "                                                                 z_t_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistrib (None, 3, 512)       279676      input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cpc_layer_2 (CPCLayer)          (None, 1)            0           lambda_2[0][0]                   \n",
      "                                                                 time_distributed_5[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 1,265,020\n",
      "Trainable params: 1,263,972\n",
      "Non-trainable params: 1,048\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-bb2a2af42ff3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mterms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mpredict_terms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-50-04a66c53f5ae>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(data, x_data, y_data, data_vis, x_data_vis, y_data_vis, n_lags, spect_height, epochs, batch_size, output_dir, code_size, lr, terms, predict_terms, color)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     )\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1424\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/miniconda3/envs/tf-conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m       \u001b[0mbatch_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36m_get_next_batch\u001b[0;34m(output_generator, mode)\u001b[0m\n\u001b[1;32m    256\u001b[0m   \u001b[0;34m\"\"\"Retrieves the next batch of input data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;31m# Returning `None` will trigger looping to stop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-conda/lib/python3.7/site-packages/tensorflow/python/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    765\u001b[0m             \u001b[0;34m'Keras requires a thread-safe generator when '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             '`use_multiprocessing=False, workers > 1`. ')\n\u001b[0;32m--> 767\u001b[0;31m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/tf-conda/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-conda/lib/python3.7/site-packages/tensorflow/python/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    741\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    744\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-conda/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    655\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-conda/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwrap_exception\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_helper_reraises_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-conda/lib/python3.7/site-packages/tensorflow/python/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mnext_sample\u001b[0;34m(uid)\u001b[0m\n\u001b[1;32m    678\u001b[0m       \u001b[0mThe\u001b[0m \u001b[0mnext\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mof\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0muid\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m   \"\"\"\n\u001b[0;32m--> 680\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_SHARED_SEQUENCES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-d4a2aac35f4b>\u001b[0m in \u001b[0;36mdata_iterator\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mshuff_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_idxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshuff_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalid_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-d4a2aac35f4b>\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdata_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    # x/y train data\n",
    "    data=data,\n",
    "    x_data=x_data,\n",
    "    y_data=y_data,\n",
    "    # x/y test data\n",
    "    data_vis=data_vis,\n",
    "    x_data_vis=x_data_vis,\n",
    "    y_data_vis=y_data_vis,\n",
    "    \n",
    "    # params\n",
    "    n_lags=2, \n",
    "    spect_height=32,\n",
    "    epochs=10,\n",
    "    batch_size=8,\n",
    "    output_dir='models/memmap',\n",
    "    code_size=512,\n",
    "    lr=2e-4,\n",
    "    terms=3,\n",
    "    predict_terms=3,\n",
    "    color=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
