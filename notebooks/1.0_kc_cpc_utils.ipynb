{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "from PIL import Image\n",
    "import scipy\n",
    "import sys\n",
    "from matplotlib import pyplot as plt\n",
    "import urllib\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rewriting MnistHandler class to handle memmap data (.dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistHandler(object):\n",
    "\n",
    "    ''' Provides a convenient interface to manipulate MNIST data '''\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        # Download data if needed\n",
    "        self.X_train, self.y_train, self.X_val, self.y_val, self.X_test, self.y_test = self.load_dataset()\n",
    "\n",
    "        # Load Lena image to memory\n",
    "        self.lena = Image.open('../resources/lena.jpg')\n",
    "\n",
    "    def load_dataset(self):\n",
    "        # Credit for this function: https://github.com/Lasagne/Lasagne/blob/master/examples/mnist.py\n",
    "\n",
    "        # We first define a download function, supporting both Python 2 and 3.\n",
    "        if sys.version_info[0] == 2:\n",
    "            from urllib import urlretrieve\n",
    "        else:\n",
    "            #from urllib.request import urlretrieve\n",
    "            import requests\n",
    "\n",
    "        def download(filename, source='http://yann.lecun.com/exdb/mnist/'):\n",
    "            print(\"Downloading %s\" % filename)\n",
    "            requests.get(source + filename, filename)\n",
    "\n",
    "        # We then define functions for loading MNIST images and labels.\n",
    "        # For convenience, they also download the requested files if needed.\n",
    "        import gzip\n",
    "\n",
    "        def load_mnist_images(filename):\n",
    "            if not os.path.exists(filename):\n",
    "                download(filename)\n",
    "            # Read the inputs in Yann LeCun's binary format.\n",
    "            with gzip.open(filename, 'rb') as f:\n",
    "                data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "            # The inputs are vectors now, we reshape them to monochrome 2D images,\n",
    "            # following the shape convention: (examples, channels, rows, columns)\n",
    "            data = data.reshape(-1, 1, 28, 28)\n",
    "            # The inputs come as bytes, we convert them to float32 in range [0,1].\n",
    "            # (Actually to range [0, 255/256], for compatibility to the version\n",
    "            # provided at http://deeplearning.net/data/mnist/mnist.pkl.gz.)\n",
    "            return data / np.float32(256)\n",
    "\n",
    "        def load_mnist_labels(filename):\n",
    "            if not os.path.exists(filename):\n",
    "                download(filename)\n",
    "            # Read the labels in Yann LeCun's binary format.\n",
    "            with gzip.open(filename, 'rb') as f:\n",
    "                data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "            # The labels are vectors of integers now, that's exactly what we want.\n",
    "            return data\n",
    "\n",
    "        # We can now download and read the training and test set images and labels.\n",
    "        X_train = load_mnist_images('../resources/train-images-idx3-ubyte.gz')\n",
    "        y_train = load_mnist_labels('../resources/train-labels-idx1-ubyte.gz')\n",
    "        X_test = load_mnist_images('../resources/t10k-images-idx3-ubyte.gz')\n",
    "        y_test = load_mnist_labels('../resources/t10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "        # We reserve the last 10000 training examples for validation.\n",
    "        X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
    "        y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
    "\n",
    "        # We just return all the arrays in order, as expected in main().\n",
    "        # (It doesn't matter how we do this as long as we can read them again.)\n",
    "        return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "    def process_batch(self, batch, batch_size, image_size=28, color=False, rescale=True):\n",
    "\n",
    "        # Resize from 28x28 to 64x64\n",
    "        if image_size == 64:\n",
    "            batch_resized = []\n",
    "            for i in range(batch.shape[0]):\n",
    "                # resize to 64x64 pixels\n",
    "                batch_resized.append(scipy.ndimage.zoom(batch[i, :, :], 2.3, order=1))\n",
    "            batch = np.stack(batch_resized)\n",
    "\n",
    "        # Convert to RGB\n",
    "        batch = batch.reshape((batch_size, 1, image_size, image_size))\n",
    "        batch = np.concatenate([batch, batch, batch], axis=1)\n",
    "\n",
    "        # Modify images if color distribution requested\n",
    "        if color:\n",
    "\n",
    "            # Binarize images\n",
    "            batch[batch >= 0.5] = 1\n",
    "            batch[batch < 0.5] = 0\n",
    "\n",
    "            # For each image in the mini batch\n",
    "            for i in range(batch_size):\n",
    "\n",
    "                # Take a random crop of the Lena image (background)\n",
    "                x_c = np.random.randint(0, self.lena.size[0] - image_size)\n",
    "                y_c = np.random.randint(0, self.lena.size[1] - image_size)\n",
    "                image = self.lena.crop((x_c, y_c, x_c + image_size, y_c + image_size))\n",
    "                image = np.asarray(image).transpose((2, 0, 1)) / 255.0\n",
    "\n",
    "                # Randomly alter the color distribution of the crop\n",
    "                for j in range(3):\n",
    "                    image[j, :, :] = (image[j, :, :] + np.random.uniform(0, 1)) / 2.0\n",
    "\n",
    "                # Invert the color of pixels where there is a number\n",
    "                image[batch[i, :, :, :] == 1] = 1 - image[batch[i, :, :, :] == 1]\n",
    "                batch[i, :, :, :] = image\n",
    "\n",
    "        # Rescale to range [-1, +1]\n",
    "        if rescale:\n",
    "            batch = batch * 2 - 1\n",
    "\n",
    "        # Channel last\n",
    "        batch = batch.transpose((0, 2, 3, 1))\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def get_batch(self, subset, batch_size, image_size=28, color=False, rescale=True):\n",
    "\n",
    "        # Select a subset\n",
    "        if subset == 'train':\n",
    "            X = self.X_train\n",
    "            y = self.y_train\n",
    "        elif subset == 'valid':\n",
    "            X = self.X_val\n",
    "            y = self.y_val\n",
    "        elif subset == 'test':\n",
    "            X = self.X_test\n",
    "            y = self.y_test\n",
    "\n",
    "        # Random choice of samples\n",
    "        idx = np.random.choice(X.shape[0], batch_size)\n",
    "        batch = X[idx, 0, :].reshape((batch_size, 28, 28))\n",
    "\n",
    "        # Process batch\n",
    "        batch = self.process_batch(batch, batch_size, image_size, color, rescale)\n",
    "\n",
    "        # Image label\n",
    "        labels = y[idx]\n",
    "\n",
    "        return batch.astype('float32'), labels.astype('int32')\n",
    "\n",
    "    def get_batch_by_labels(self, subset, labels, image_size=28, color=False, rescale=True):\n",
    "\n",
    "        # Select a subset\n",
    "        if subset == 'train':\n",
    "            X = self.X_train\n",
    "            y = self.y_train\n",
    "        elif subset == 'valid':\n",
    "            X = self.X_val\n",
    "            y = self.y_val\n",
    "        elif subset == 'test':\n",
    "            X = self.X_test\n",
    "            y = self.y_test\n",
    "\n",
    "        # Find samples matching labels\n",
    "        idxs = []\n",
    "        for i, label in enumerate(labels):\n",
    "\n",
    "            idx = np.where(y == label)[0]\n",
    "            idx_sel = np.random.choice(idx, 1)[0]\n",
    "            idxs.append(idx_sel)\n",
    "\n",
    "        # Retrieve images\n",
    "        batch = X[np.array(idxs), 0, :].reshape((len(labels), 28, 28))\n",
    "\n",
    "        # Process batch\n",
    "        batch = self.process_batch(batch, len(labels), image_size, color, rescale)\n",
    "\n",
    "        return batch.astype('float32'), labels.astype('int32')\n",
    "\n",
    "    def get_n_samples(self, subset):\n",
    "\n",
    "        if subset == 'train':\n",
    "            y_len = self.y_train.shape[0]\n",
    "        elif subset == 'valid':\n",
    "            y_len = self.y_val.shape[0]\n",
    "        elif subset == 'test':\n",
    "            y_len = self.y_test.shape[0]\n",
    "\n",
    "        return y_len\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemmapHandler(object):\n",
    "\n",
    "    ''' Provides a convenient interface to manipulate memmap data (.dat) '''\n",
    "    \n",
    "    def __init__(self):\n",
    "         self.X_train, self.y_train, self.X_val, self.y_val, self.X_test, self.y_test = self.load_memmap_data()\n",
    "\n",
    "    def load_memmap_data(dirname):\n",
    "        if not os.path.exists(dirname):\n",
    "            print(\"cannot find memmap data.\")\n",
    "        # read in directory of .dat files.\n",
    "        file_List = os.listdir(dirname)\n",
    "        for file in file_list:\n",
    "            file = np.fromfile('%sx_lag%03d.dat' % (datafolder, n_lags), dtype=dt)\n",
    "            data = np.frombuffer(file.read(), np.uint8, offset=16)\n",
    "        # The inputs are vectors now, we reshape them to monochrome 2D images,\n",
    "        # following the shape convention: (examples, channels, rows, columns)\n",
    "        data = data.reshape(-1, 1, 28, 28)\n",
    "        # The inputs come as bytes, we convert them to float32 in range [0,1].\n",
    "        # (Actually to range [0, 255/256], for compatibility to the version\n",
    "        # provided at http://deeplearning.net/data/mnist/mnist.pkl.gz.)\n",
    "        return data / np.float32(256)\n",
    "\n",
    "\n",
    "        # We can now download and read the training and test set images and labels.\n",
    "        train = load_memmap_data('../resources/memmap_dataset_stimulus/')\n",
    "        test = load_memmap_data('../resources/memmap_dataset_stimulus_vis/')\n",
    "        \n",
    "        # 80/20 train/test split\n",
    "        # reserve additional 10% of training files for validation (2 files)\n",
    "        X_train, X_test = train[:-2], train[-2:]\n",
    "        y_train, y_test = test[:-2], test[-2:]\n",
    "\n",
    "        # We reserve the last 2 training examples for validation.\n",
    "        X_train, X_val = X_train[:-2], X_train[-2:]\n",
    "        y_train, y_val = y_train[:-2], y_train[-2:]\n",
    "\n",
    "        # We just return all the arrays in order, as expected in main().\n",
    "        # (It doesn't matter how we do this as long as we can read them again.)\n",
    "        return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistGenerator(object):\n",
    "\n",
    "    ''' Data generator providing MNIST data '''\n",
    "\n",
    "    def __init__(self, batch_size, subset, image_size=28, color=False, rescale=True):\n",
    "\n",
    "        # Set params\n",
    "        self.batch_size = batch_size\n",
    "        self.subset = subset\n",
    "        self.image_size = image_size\n",
    "        self.color = color\n",
    "        self.rescale = rescale\n",
    "\n",
    "        # Initialize MNIST dataset\n",
    "        self.mnist_handler = MnistHandler()\n",
    "        self.n_samples = self.mnist_handler.get_n_samples(subset)\n",
    "        self.n_batches = self.n_samples // batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_batches\n",
    "\n",
    "    def next(self):\n",
    "\n",
    "        # Get data\n",
    "        x, y = self.mnist_handler.get_batch(self.subset, self.batch_size, self.image_size, self.color, self.rescale)\n",
    "        \n",
    "        # Convert y to one-hot\n",
    "        y_h = np.eye(10)[y]\n",
    "        print(x.shape, y_h.shape)\n",
    "        print(y_h)\n",
    "\n",
    "        return x, y_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SortedNumberGenerator(object):\n",
    "\n",
    "    ''' Data generator providing lists of sorted numbers '''\n",
    "\n",
    "    def __init__(self, batch_size, subset, terms, positive_samples=1, predict_terms=1, image_size=28, color=False, rescale=True):\n",
    "\n",
    "        # Set params\n",
    "        self.positive_samples = positive_samples\n",
    "        self.predict_terms = predict_terms\n",
    "        self.batch_size = batch_size\n",
    "        self.subset = subset\n",
    "        self.terms = terms\n",
    "        self.image_size = image_size\n",
    "        self.color = color\n",
    "        self.rescale = rescale\n",
    "\n",
    "        # Initialize MNIST dataset\n",
    "        self.mnist_handler = MnistHandler()\n",
    "        self.n_samples = self.mnist_handler.get_n_samples(subset) // terms\n",
    "        self.n_batches = self.n_samples // batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_batches\n",
    "\n",
    "    def next(self):\n",
    "\n",
    "        # Build sentences\n",
    "        image_labels = np.zeros((self.batch_size, self.terms + self.predict_terms))\n",
    "        sentence_labels = np.ones((self.batch_size, 1)).astype('int32')\n",
    "        positive_samples_n = self.positive_samples\n",
    "        for b in range(self.batch_size):\n",
    "\n",
    "            # Set ordered predictions for positive samples\n",
    "            seed = np.random.randint(0, 10)\n",
    "            sentence = np.mod(np.arange(seed, seed + self.terms + self.predict_terms), 10)\n",
    "\n",
    "            if positive_samples_n <= 0:\n",
    "\n",
    "                # Set random predictions for negative samples\n",
    "                # Each predicted term draws a number from a distribution that excludes itself\n",
    "                numbers = np.arange(0, 10)\n",
    "                predicted_terms = sentence[-self.predict_terms:]\n",
    "                for i, p in enumerate(predicted_terms):\n",
    "                    predicted_terms[i] = np.random.choice(numbers[numbers != p], 1)\n",
    "                sentence[-self.predict_terms:] = np.mod(predicted_terms, 10)\n",
    "                sentence_labels[b, :] = 0\n",
    "\n",
    "            # Save sentence\n",
    "            image_labels[b, :] = sentence\n",
    "\n",
    "            positive_samples_n -= 1\n",
    "\n",
    "        # Retrieve actual images\n",
    "        images, _ = self.mnist_handler.get_batch_by_labels(self.subset, image_labels.flatten(), self.image_size, self.color, self.rescale)\n",
    "\n",
    "        # Assemble batch\n",
    "        images = images.reshape((self.batch_size, self.terms + self.predict_terms, images.shape[1], images.shape[2], images.shape[3]))\n",
    "        x_images = images[:, :-self.predict_terms, ...]\n",
    "        y_images = images[:, -self.predict_terms:, ...]\n",
    "        print(\"x_img: \", x_images.shape)\n",
    "        print(\"y_img: \", y_images.shape)\n",
    "\n",
    "        # Randomize\n",
    "        idxs = np.random.choice(sentence_labels.shape[0], sentence_labels.shape[0], replace=False)\n",
    "        print(idxs, idxs.shape)\n",
    "        \n",
    "        return [x_images[idxs, ...], y_images[idxs, ...]], sentence_labels[idxs, ...], idxs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing *all* these objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 28, 28, 3) (8, 10)\n",
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "(8, 28, 28, 3) (8, 10)\n",
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "(8, 28, 28, 3) (8, 10)\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-17aaee917525>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test MnistGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMnistGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrescale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Test MnistGenerator\n",
    "mh, mx = MnistGenerator(batch_size=8, subset='train', image_size=28, color=False, rescale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MnistGenerator at 0x7ff36f30a390>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_img:  (8, 4, 64, 64, 3)\n",
      "y_img:  (8, 4, 64, 64, 3)\n",
      "[1 0 6 3 7 2 5 4] (8,)\n",
      "x_img:  (8, 4, 64, 64, 3)\n",
      "y_img:  (8, 4, 64, 64, 3)\n",
      "[0 4 5 3 1 2 6 7] (8,)\n",
      "x_img:  (8, 4, 64, 64, 3)\n",
      "y_img:  (8, 4, 64, 64, 3)\n",
      "[0 7 5 2 1 4 3 6] (8,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-9e9be23de983>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test SortedNumberGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mSortedNumberGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_terms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrescale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m '''\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Test SortedNumberGenerator\n",
    "ag, idx= SortedNumberGenerator(batch_size=8, subset='train', terms=4, positive_samples=4, predict_terms=4, image_size=64, color=True, rescale=False)\n",
    "\n",
    "\n",
    "'''\n",
    "for (x, y), labels in ag:\n",
    "    plot_sequences(x, y, labels, output_path=r'resources/memmap_sorted.png')\n",
    "    break\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SortedNumberGenerator output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1562"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = next(ag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = a[0]\n",
    "img_lbls = a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 4, 64, 64, 3)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of images (x)\n",
    "np.vstack(imgs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 1)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of image labels (y)\n",
    "np.vstack(img_lbls).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 4, 64, 64, 3)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "messing with train and validation sets in train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#from data_utils import SortedNumberGenerator\n",
    "from os.path import join, basename, dirname, exists\n",
    "import keras\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/AD/kachiem/miniconda3/envs/tf-conda/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 64, 64, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 31, 31, 64)        1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 31, 31, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 31, 31, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 15, 15, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 15, 15, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 7, 7, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 3, 3, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               147712    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "encoder_embedding (Dense)    (None, 128)               32896     \n",
      "=================================================================\n",
      "Total params: 295,232\n",
      "Trainable params: 294,208\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 4, 64, 64, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 4, 128)       295232      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "ar_context (GRU)                (None, 256)          295680      time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "z_t_0 (Dense)                   (None, 128)          32896       ar_context[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "z_t_1 (Dense)                   (None, 128)          32896       ar_context[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "z_t_2 (Dense)                   (None, 128)          32896       ar_context[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "z_t_3 (Dense)                   (None, 128)          32896       ar_context[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 4, 64, 64, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 4, 128)       0           z_t_0[0][0]                      \n",
      "                                                                 z_t_1[0][0]                      \n",
      "                                                                 z_t_2[0][0]                      \n",
      "                                                                 z_t_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 4, 128)       295232      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cpc_layer_1 (CPCLayer)          (None, 1)            0           lambda_1[0][0]                   \n",
      "                                                                 time_distributed_2[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 722,496\n",
      "Trainable params: 721,472\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From /home/AD/kachiem/miniconda3/envs/tf-conda/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "x_img:  (32, 4, 64, 64, 3)\n",
      "y_img:  (32, 4, 64, 64, 3)\n",
      "[12 15 22 11 27 23 25  2 13  7  8 18 29  5 21  3  6  1 16 24 17  0 26  9\n",
      " 10 19  4 14 20 28 31 30] (32,)\n",
      "x_img:  (32, 4, 64, 64, 3)\n",
      "y_img:  (32, 4, 64, 64, 3)\n",
      "[30 13 11 19  1  5  6 31 23  8 18 15 27 14 17 22 20 25 28  4  3  9 21 12\n",
      " 24 26  0 10 16  2  7 29] (32,)\n",
      "x_img:  (32, 4, 64, 64, 3)\n",
      "y_img:  (32, 4, 64, 64, 3)\n",
      "[29 22 14  9 25  7 18 10 20  3  2  4 23  8 16 31  6 26 12 13 24 28 21 17\n",
      " 19 30  5  0 27 11 15  1] (32,)\n",
      "x_img:  (32, 4, 64, 64, 3)\n",
      "y_img:  (32, 4, 64, 64, 3)\n",
      "[28 12 20 18 29 19 30 15  0 13 14 25 24  7  5  4  6  1 27 21  8 16 11  9\n",
      " 10 26 22 17 23 31  3  2] (32,)\n",
      "x_img:  (32, 4, 64, 64, 3)\n",
      "y_img:  (32, 4, 64, 64, 3)\n",
      "[25 29  4  5 23 19 20 27 16 31  7  8  0 21 28  6  1 14  9 30 10 15 11 13\n",
      "  3 26 12  2 17 22 24 18] (32,)\n",
      "x_img:  (32, 4, 64, 64, 3)\n",
      "y_img:  (32, 4, 64, 64, 3)\n",
      "[26 20  9  5 13 14 30 15  0 21  7 16 25  6 22  3  4  1 10 12 29  2 28 19\n",
      " 27 17 31 11 18  8 24 23] (32,)\n",
      "x_img:  (32, 4, 64, 64, 3)\n",
      "y_img:  (32, 4, 64, 64, 3)\n",
      "[23 10 24 16 17  2  6 20 12 28 13  9 22 14 27 26 18  7 11 29 25 31  5  3\n",
      " 21  8  4  0  1 15 30 19] (32,)\n",
      "x_img:  (32, 4, 64, 64, 3)\n",
      "y_img:  (32, 4, 64, 64, 3)\n",
      "[ 8 14 27 31 25  6 18  2 16 11  3 29  9 10 26 21  7 13  5 12 17 22  0  4\n",
      " 15 19 20 28 24  1 30 23] (32,)\n",
      "x_img:  (32, 4, 64, 64, 3)\n",
      "y_img:  (32, 4, 64, 64, 3)\n",
      "[12 25 27 31 14  2 19 30  6  4 26 24  3 20  8 10 29 28 13 21  5  9 18 15\n",
      " 11 17 16  7  0 22 23  1] (32,)\n",
      "x_img:  (32, 4, 64, 64, 3)\n",
      "y_img:  (32, 4, 64, 64, 3)\n",
      "[28 12 10 15 18 14 23  6  4 20  3 11 31 27 17  5  2  7 16 22  9 19 24  8\n",
      "  0 30 29 13  1 21 25 26] (32,)\n",
      "x_img:  (32, 4, 64, 64, 3)\n",
      "y_img:  (32, 4, 64, 64, 3)\n",
      "[14 26 16 17 30 28  0  1 20  9 18  5  8  7 12 19 25 31 10 13 23 29 11  6\n",
      " 27  2 22  4 21 15 24  3] (32,)\n",
      "x_img:  (32, 4, 64, 64, 3)\n",
      "y_img:  (32, 4, 64, 64, 3)\n",
      "[11 15 10  1  3 26 31  8 14 18  9 21  2 30 28 23  7 13 27 19  0  4 22 24\n",
      "  6 12 16  5 17 20 29 25] (32,)\n",
      "x_img:  (32, 4, 64, 64, 3)\n",
      "y_img:  (32, 4, 64, 64, 3)\n",
      "[21 30  1  8 18  9 14 29 28 26 24 15  4 27 23  3  5  2  6 13 16 20  7 25\n",
      " 11 12 19 31 22 10  0 17] (32,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_img:  (32, 4, 64, 64, 3)\n",
      "y_img:  (32, 4, 64, 64, 3)\n",
      "[31  5 27 17 19 12  9 20 16 13  2 11 30 26  8 25  6 29  4  7  3 21 14 18\n",
      " 23 15 28  1 22 10 24  0] (32,)\n",
      "x_img:  (32, 4, 64, 64, 3)\n",
      "y_img:  (32, 4, 64, 64, 3)\n",
      "[13 10 11  8 29  2 24  0 25 30 12  4 23 17 31  5 28 21 27  1  6 22  3 14\n",
      " 15 16  7 18 19 26  9 20] (32,)\n",
      "x_img:  (32, 4, 64, 64, 3)\n",
      "y_img:  (32, 4, 64, 64, 3)\n",
      "[ 3 31 10 17  9 14 25 16 11  0  5 13  2  6 19 12 27 15  1  8 22 21 23  7\n",
      "  4 30 18 20 28 26 24 29] (32,)\n",
      "x_img:  (32, 4, 64, 64, 3)\n",
      "y_img:  (32, 4, 64, 64, 3)\n",
      "[ 4 25 21 23  5 24 18 11  2 14  9 31 22 28  3  8 27 16 17  7 13  6 29 30\n",
      " 20 10  0 15 26  1 12 19] (32,)\n",
      "x_img:  (32, 4, 64, 64, 3)\n",
      "y_img:  (32, 4, 64, 64, 3)\n",
      "[26 31 13 10 16 11  6 21  3  8  1 25  9  7  0 28 12 30  2 15 17 20 19 22\n",
      " 24  5 29 23 27 18 14  4] (32,)\n",
      "x_img:  (32, 4, 64, 64, 3)\n",
      "y_img:  (32, 4, 64, 64, 3)\n",
      "[22  3 19  2 31 13 12  8 27 16  6 29 25 28 18 26  9 30 23  4 20 17 14 15\n",
      " 24 10  7  1 11  5  0 21] (32,)\n",
      "x_img:  (32, 4, 64, 64, 3)\n",
      "y_img:  (32, 4, 64, 64, 3)\n",
      "[ 4  6 20 11 26 10 28  5 18  1 13  9 12  2 25 24  0 14 30 16 17 19 29 15\n",
      "  7  3 31 27 22  8 23 21] (32,)\n",
      "x_img:  (32, 4, 64, 64, 3)\n",
      "y_img:  (32, 4, 64, 64, 3)\n",
      "[ 6  1 11  7 24 13 25 27 19  4 18 31  5 14  0 10  8 30 26 22  9 17 28 29\n",
      " 15  3 16  2 20 23 21 12] (32,)\n",
      "x_img:  (32, 4, 64, 64, 3)\n",
      "y_img:  (32, 4, 64, 64, 3)\n",
      "[10  9 24  2 28  7 20 17  1 12 13 26 18 23 11 29  6 22  5  0  8 16 21 27\n",
      " 15 25  3 30 14  4 19 31] (32,)\n",
      "x_img:  (32, 4, 64, 64, 3)\n",
      "y_img:  (32, 4, 64, 64, 3)\n",
      "[25 27  6  9 24 11 17 14 18 13  3  1 22 20 19 28 12 31 21  5  8 23  4  7\n",
      "  0 16 26 15 10 29  2 30] (32,)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-465f1b1e1694>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpredict_terms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-65-27af98d28f73>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(epochs, batch_size, output_dir, code_size, lr, terms, predict_terms, image_size, color)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     )\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-conda/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-conda/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-conda/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-conda/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-conda/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "    train_model(\n",
    "        epochs=10,\n",
    "        batch_size=32,\n",
    "        output_dir='models/64x64',\n",
    "        code_size=128,\n",
    "        lr=1e-3,\n",
    "        terms=4,\n",
    "        predict_terms=4,\n",
    "        image_size=64,\n",
    "        color=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_encoder(x, code_size):\n",
    "\n",
    "    ''' Define the network mapping images to embeddings '''\n",
    "\n",
    "    x = keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, activation='linear')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.LeakyReLU()(x)\n",
    "    x = keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, activation='linear')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.LeakyReLU()(x)\n",
    "    x = keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, activation='linear')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.LeakyReLU()(x)\n",
    "    x = keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, activation='linear')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.LeakyReLU()(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Dense(units=256, activation='linear')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.LeakyReLU()(x)\n",
    "    x = keras.layers.Dense(units=code_size, activation='linear', name='encoder_embedding')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def network_autoregressive(x):\n",
    "\n",
    "    ''' Define the network that integrates information along the sequence '''\n",
    "\n",
    "    # x = keras.layers.GRU(units=256, return_sequences=True)(x)\n",
    "    # x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.GRU(units=256, return_sequences=False, name='ar_context')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def network_prediction(context, code_size, predict_terms):\n",
    "\n",
    "    ''' Define the network mapping context to multiple embeddings '''\n",
    "\n",
    "    outputs = []\n",
    "    for i in range(predict_terms):\n",
    "        outputs.append(keras.layers.Dense(units=code_size, activation=\"linear\", name='z_t_{i}'.format(i=i))(context))\n",
    "\n",
    "    if len(outputs) == 1:\n",
    "        output = keras.layers.Lambda(lambda x: K.expand_dims(x, axis=1))(outputs[0])\n",
    "    else:\n",
    "        output = keras.layers.Lambda(lambda x: K.stack(x, axis=1))(outputs)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "class CPCLayer(keras.layers.Layer):\n",
    "\n",
    "    ''' Computes dot product between true and predicted embedding vectors '''\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CPCLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        # Compute dot product among vectors\n",
    "        preds, y_encoded = inputs\n",
    "        dot_product = K.mean(y_encoded * preds, axis=-1)\n",
    "        dot_product = K.mean(dot_product, axis=-1, keepdims=True)  # average along the temporal dimension\n",
    "\n",
    "        # Keras loss functions take probabilities\n",
    "        dot_product_probs = K.sigmoid(dot_product)\n",
    "\n",
    "        return dot_product_probs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], 1)\n",
    "\n",
    "\n",
    "def network_cpc(image_shape, terms, predict_terms, code_size, learning_rate):\n",
    "\n",
    "    ''' Define the CPC network combining encoder and autoregressive model '''\n",
    "\n",
    "    # Set learning phase (https://stackoverflow.com/questions/42969779/keras-error-you-must-feed-a-value-for-placeholder-tensor-bidirectional-1-keras)\n",
    "    K.set_learning_phase(1)\n",
    "\n",
    "    # Define encoder model\n",
    "    encoder_input = keras.layers.Input(image_shape)\n",
    "    encoder_output = network_encoder(encoder_input, code_size)\n",
    "    encoder_model = keras.models.Model(encoder_input, encoder_output, name='encoder')\n",
    "    encoder_model.summary()\n",
    "\n",
    "    # Define rest of model\n",
    "    x_input = keras.layers.Input((terms, image_shape[0], image_shape[1], image_shape[2]))\n",
    "    x_encoded = keras.layers.TimeDistributed(encoder_model)(x_input)\n",
    "    context = network_autoregressive(x_encoded)\n",
    "    preds = network_prediction(context, code_size, predict_terms)\n",
    "\n",
    "    y_input = keras.layers.Input((predict_terms, image_shape[0], image_shape[1], image_shape[2]))\n",
    "    y_encoded = keras.layers.TimeDistributed(encoder_model)(y_input)\n",
    "\n",
    "    # Loss\n",
    "    dot_product_probs = CPCLayer()([preds, y_encoded])\n",
    "\n",
    "    # Model\n",
    "    cpc_model = keras.models.Model(inputs=[x_input, y_input], outputs=dot_product_probs)\n",
    "\n",
    "    # Compile model\n",
    "    cpc_model.compile(\n",
    "        optimizer=keras.optimizers.Adam(lr=learning_rate),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['binary_accuracy']\n",
    "    )\n",
    "    cpc_model.summary()\n",
    "\n",
    "    return cpc_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs, batch_size, output_dir, code_size, lr=1e-4, terms=4, predict_terms=4, image_size=28, color=False):\n",
    "\n",
    "    # Prepare data\n",
    "    train_data = SortedNumberGenerator(batch_size=batch_size, subset='train', terms=terms,\n",
    "                                       positive_samples=batch_size // 2, predict_terms=predict_terms,\n",
    "                                       image_size=image_size, color=color, rescale=True)\n",
    "\n",
    "    validation_data = SortedNumberGenerator(batch_size=batch_size, subset='valid', terms=terms,\n",
    "                                            positive_samples=batch_size // 2, predict_terms=predict_terms,\n",
    "                                            image_size=image_size, color=color, rescale=True)\n",
    "\n",
    "    # Prepares the model\n",
    "    model = network_cpc(image_shape=(image_size, image_size, 3), terms=terms, predict_terms=predict_terms,\n",
    "                        code_size=code_size, learning_rate=lr)\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = [keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=1/3, patience=2, min_lr=1e-4)]\n",
    "\n",
    "    # Trains the model\n",
    "    model.fit_generator(\n",
    "        generator=train_data,\n",
    "        steps_per_epoch=len(train_data),\n",
    "        validation_data=validation_data,\n",
    "        validation_steps=len(validation_data),\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    # Saves the model\n",
    "    # Remember to add custom_objects={'CPCLayer': CPCLayer} to load_model when loading from disk\n",
    "    model.save(join(output_dir, 'cpc.h5'))\n",
    "\n",
    "    # Saves the encoder alone\n",
    "    encoder = model.layers[1].layer\n",
    "    encoder.save(join(output_dir, 'encoder.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
