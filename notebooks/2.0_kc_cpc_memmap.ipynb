{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/AD/kachiem/miniconda3/envs/tf-conda/bin/python3\n",
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob as glob\n",
    "from os.path import join, basename, dirname, exists\n",
    "import sys\n",
    "print(sys.executable)  #print kernel path\n",
    "#print(sys.path)\n",
    "\n",
    "# tensorflow imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "print(tf.__version__)\n",
    "\n",
    "# custom imports\n",
    "import librispect as lspct\n",
    "from librispect.features import predict\n",
    "from librispect.utils import split_validation\n",
    "\n",
    "# cpc import\n",
    "sys.path.insert(1, '../')\n",
    "from data_utils import SortedNumberGenerator\n",
    "sys.path.insert(1, 'notebooks/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attempting to connect srihita's spectrogram function to train_model  \n",
    "- first, the network itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_encoder(x, code_size):\n",
    "\n",
    "    ''' Define the network mapping images to embeddings '''\n",
    "\n",
    "    x = keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, activation='linear')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.LeakyReLU()(x)\n",
    "    x = keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, activation='linear')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.LeakyReLU()(x)\n",
    "    x = keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, activation='linear')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.LeakyReLU()(x)\n",
    "    x = keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, activation='linear')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.LeakyReLU()(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Dense(units=256, activation='linear')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.LeakyReLU()(x)\n",
    "    x = keras.layers.Dense(units=code_size, activation='linear', name='encoder_embedding')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def network_autoregressive(x):\n",
    "\n",
    "    ''' Define the network that integrates information along the sequence '''\n",
    "\n",
    "    # x = keras.layers.GRU(units=256, return_sequences=True)(x)\n",
    "    # x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.GRU(units=256, return_sequences=False, name='ar_context')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def network_prediction(context, code_size, predict_terms):\n",
    "\n",
    "    ''' Define the network mapping context to multiple embeddings '''\n",
    "\n",
    "    outputs = []\n",
    "    for i in range(predict_terms):\n",
    "        outputs.append(keras.layers.Dense(units=code_size, activation=\"linear\", name='z_t_{i}'.format(i=i))(context))\n",
    "\n",
    "    if len(outputs) == 1:\n",
    "        output = keras.layers.Lambda(lambda x: K.expand_dims(x, axis=1))(outputs[0])\n",
    "    else:\n",
    "        output = keras.layers.Lambda(lambda x: K.stack(x, axis=1))(outputs)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "class CPCLayer(keras.layers.Layer):\n",
    "\n",
    "    ''' Computes dot product between true and predicted embedding vectors '''\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CPCLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        # Compute dot product among vectors\n",
    "        preds, y_encoded = inputs\n",
    "        dot_product = K.mean(y_encoded * preds, axis=-1)\n",
    "        dot_product = K.mean(dot_product, axis=-1, keepdims=True)  # average along the temporal dimension\n",
    "\n",
    "        # Keras loss functions take probabilities\n",
    "        dot_product_probs = K.sigmoid(dot_product)\n",
    "\n",
    "        return dot_product_probs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], 1)\n",
    "\n",
    "\n",
    "def network_cpc(image_shape, terms, predict_terms, code_size, learning_rate):\n",
    "\n",
    "    ''' Define the CPC network combining encoder and autoregressive model '''\n",
    "\n",
    "    # Set learning phase (https://stackoverflow.com/questions/42969779/keras-error-you-must-feed-a-value-for-placeholder-tensor-bidirectional-1-keras)\n",
    "    K.set_learning_phase(1)\n",
    "\n",
    "    # Define encoder model\n",
    "    encoder_input = keras.layers.Input(image_shape)\n",
    "    encoder_output = network_encoder(encoder_input, code_size)\n",
    "    encoder_model = keras.models.Model(encoder_input, encoder_output, name='encoder')\n",
    "    encoder_model.summary()\n",
    "\n",
    "    # Define rest of model\n",
    "    x_input = keras.layers.Input((terms, image_shape[0], image_shape[1], image_shape[2]))\n",
    "    x_encoded = keras.layers.TimeDistributed(encoder_model)(x_input)\n",
    "    context = network_autoregressive(x_encoded)\n",
    "    preds = network_prediction(context, code_size, predict_terms)\n",
    "\n",
    "    y_input = keras.layers.Input((predict_terms, image_shape[0], image_shape[1], image_shape[2]))\n",
    "    y_encoded = keras.layers.TimeDistributed(encoder_model)(y_input)\n",
    "\n",
    "    # Loss\n",
    "    dot_product_probs = CPCLayer()([preds, y_encoded])\n",
    "\n",
    "    # Model\n",
    "    cpc_model = keras.models.Model(inputs=[x_input, y_input], outputs=dot_product_probs)\n",
    "\n",
    "    # Compile model\n",
    "    cpc_model.compile(\n",
    "        optimizer=keras.optimizers.Adam(lr=learning_rate),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['binary_accuracy']\n",
    "    )\n",
    "    cpc_model.summary()\n",
    "\n",
    "    return cpc_model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keep scrollin'!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shaping input for network - spectrogram functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Instead of images and labels, I have spectrogram segments x and y.\n",
    "So x is input and y would be output (aka labels). \n",
    "You train on bunch of (x - power,y - number of samples), let's call it train_x and train_y.\n",
    "Then when you input test_x, you should get test_y (or vis_x and vis_y in this case)\n",
    "'''\n",
    "\n",
    "# spect_height = number of freq. bins\n",
    "# y always has one time bin\n",
    "# n_lags is number of time bins of x\n",
    "# shape of x = (n_lags,spect_height) x number of samples\n",
    "# shape of y = (1, spect_height) x number of samples\n",
    "\n",
    "n_lags = 2\n",
    "spect_height = 32\n",
    "NIN = n_lags*spect_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location\n",
    "datafolder = '/home/AD/kachiem/memmap/memmap_dataset_stimulus/'\n",
    "visfolder = '/home/AD/kachiem/memmap/memmap_dataset_stimulus_vis/'\n",
    "x_loc = '%sx_lag%03d.dat' % (datafolder, n_lags)\n",
    "y_loc = '%sy_lag%03d.dat' % (datafolder, n_lags)\n",
    "x_loc_vis = '%sx_lag%03d.dat' % (visfolder, n_lags)\n",
    "y_loc_vis = '%sy_lag%03d.dat' % (visfolder, n_lags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### creating train data  (cell converted to markdown to prevent running)\n",
    "y = np.memmap(y_loc, dtype='float32', mode='r')\n",
    "num_data_samples = int(len(y) / spect_height)\n",
    "\n",
    "x_data = np.memmap(x_loc, dtype='float32', mode='r+', shape=(n_lags*spect_height, num_data_samples))\n",
    "y_data = np.memmap(y_loc, dtype='float32', mode='r+', shape=(spect_height, num_data_samples))\n",
    "\n",
    "x_data.shape, y_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### saving train data\n",
    "np.save('y.npy', y)\n",
    "np.save('x_data.npy', x_data)\n",
    "np.save('y_data.npy', y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adapting srihita's spectrogram functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generating test data\n",
    "y_data_vis = np.memmap(y_loc_vis, dtype='float32', mode='r')\n",
    "num_data_samples_vis = int(len(y_data_vis) / spect_height)\n",
    "\n",
    "x_data_vis = np.memmap(x_loc_vis, dtype='float32', mode='r', shape=(n_lags*spect_height, num_data_samples_vis))\n",
    "y_data_vis = np.memmap(y_loc_vis, dtype='float32', mode='r', shape=(spect_height, num_data_samples_vis))\n",
    "x_data_vis.shape, y_data_vis.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### saving test data\n",
    "np.save('y_vis.npy', y_data_vis)\n",
    "np.save('x_data_vis.npy', x_data_vis)\n",
    "np.save('y_data_vis.npy', y_data_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args for the following class, MemmapHandler\n",
    "datafolder = '/home/AD/kachiem/memmap/memmap_dataset_stimulus/'\n",
    "visfolder = '/home/AD/kachiem/memmap/memmap_dataset_stimulus_vis/'\n",
    "n_lags = 2\n",
    "spect_height = 32\n",
    "#subset = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MemmapHandler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-4e1b3768ab0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMemmapHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'MemmapHandler' is not defined"
     ]
    }
   ],
   "source": [
    "mh = MemmapHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs WERK\n",
    "class MemmapHandler(object):\n",
    "    ''' making and saving memmaps '''\n",
    "    def __init__(self):\n",
    "        # args:  folder, n_lags, spect_height, subset\n",
    "        \n",
    "        # spect_height = number of freq. bins\n",
    "        # y always has one time bin\n",
    "        # n_lags is number of time bins of x\n",
    "        # shape of x = (n_lags,spect_height) x number of samples\n",
    "        # shape of y = (1, spect_height) x number of samples\n",
    "        \n",
    "        # input variables\n",
    "        self.n_lags = 2\n",
    "        self.spect_height = 32\n",
    "        self.subset = ''  # train/valid/test\n",
    "        \n",
    "        # generated variables\n",
    "        self.NIN = self.n_lags*self.spect_height\n",
    "        self.train, self.sub_train, self.test, self.sub_test = self.init_data()\n",
    "        #self.train, self.sub_train, self.valid, self.sub_valid, self.test, self.sub_test = self.init_data()\n",
    "        self.num_data_samples = self.get_n_samples(self.subset)\n",
    "        self.shape = (n_lags*spect_height, self.num_data_samples)\n",
    "        \n",
    "        def init_data(self):\n",
    "            # args: folder, name, data\n",
    "            ''' Create intitial stimulus and stimulus_vis memmaps '''\n",
    "            from pathlib import Path\n",
    "\n",
    "            def read_file_loc(self, folder, n_lags):\n",
    "                ''' function to read in name of .dat file '''\n",
    "                loc = '%sx_lag%03d.dat' % (folder, n_lags)\n",
    "                return loc\n",
    "\n",
    "            def if_file(folder):\n",
    "                # check if stimulus or stimulus_vis\n",
    "                if folder.contains('vis'):\n",
    "                    filename = 'memmap_vis.npy'\n",
    "                    subset = 'test'\n",
    "                    f_loc = read_file_loc(folder, self.n_lags)\n",
    "                else:\n",
    "                    filename = 'memmap.npy'\n",
    "                    subset = 'train'\n",
    "                    f_loc = read_file_loc(folder, self.n_lags)\n",
    "\n",
    "                filepath = Path(folder + filename)\n",
    "\n",
    "                # check if file exists, and either load or generate.\n",
    "                if data_file.is_file():\n",
    "                    data = load_memmap(filename)\n",
    "                else:\n",
    "                    data = generate_memmap(filename, f_loc, shape)\n",
    "                    sav_data = save_memmap(filepath, f_loc)\n",
    "                return data, subset\n",
    "            \n",
    "            def get_train_set(source='/home/AD/kachiem/memmap/memmap_dataset_stimulus/'):\n",
    "                train_set, subset = if_file(source, data)\n",
    "                return train_set, subset\n",
    "\n",
    "            def get_test_set(source='/home/AD/kachiem/memmap/memmap_dataset_stimulus_vis/'):\n",
    "                test_set, subset = if_file(source, data)\n",
    "                return test_set, subset\n",
    "            \n",
    "            \n",
    "            train_set, train_sub = get_train_set()\n",
    "            test_set, test_sub = get_test_set()\n",
    "            \n",
    "            return train_set, train_sub, test_set, test_sub\n",
    "            \n",
    "            \n",
    "        \n",
    "        def get_n_samples(self, data, subset):\n",
    "            ''' function to get number of samples in subset '''\n",
    "            if subset == 'train':\n",
    "                return int(len(data) / self.spect_height)\n",
    "            elif subset == 'test':\n",
    "                return int(len(data) / self.spect_height)\n",
    "        \n",
    "        \n",
    "        def generate_memmap(self, data, shape):\n",
    "            new_map = np.memmap(data, dtype='float32', mode='r', shape=shape)\n",
    "            return new_map\n",
    "        \n",
    "        def save_memmap(self, folder, filename, data):\n",
    "            memmap = np.save(folder + filename, data)\n",
    "            return memmap\n",
    "            \n",
    "        def load_memmap(self, filename):\n",
    "            saved_map = np.lib.format.open_memmap(filename, dtype='float32', mode='r', shape=shape)\n",
    "            return saved_map\n",
    "            \n",
    "        def memmap_copy(self, memmap_in, start_idx, end_idx):\n",
    "            ''' function to copy memmap and save ram !! '''\n",
    "            memmap_copy = memmap_in[:]\n",
    "            subset = memmap_copy[start_idx, end_idx]\n",
    "            return subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memmap_copy(self, memmap_in, start_idx, end_idx):\n",
    "    ''' function to copy memmap and save ram !! '''\n",
    "    memmap_copy = memmap_in[:]\n",
    "    subset = memmap_copy[start_idx, end_idx]\n",
    "    return subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((64, 149962), (32, 149962))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading train data\n",
    "why = np.lib.format.open_memmap('y.npy', dtype='float32', mode='r')\n",
    "num_data_samples = int(len(why) / spect_height)\n",
    "xdata = np.lib.format.open_memmap('x_data.npy', dtype='float32', mode='r', shape=(n_lags*spect_height, num_data_samples))\n",
    "ydata = np.lib.format.open_memmap('y_data.npy', dtype='float32', mode='r', shape=(spect_height, num_data_samples))\n",
    "\n",
    "xdata.shape, ydata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set: memmap inputs\n",
    "data = why[:]\n",
    "x_data = xdata[:, :1000]\n",
    "y_data = ydata[:, :1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((64, 149962), (32, 149962))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading test data\n",
    "why_vis = np.lib.format.open_memmap('y_vis.npy', dtype='float32', mode='r')\n",
    "num_data_samples_vis = int(len(why_vis) / spect_height)\n",
    "xdata_vis = np.lib.format.open_memmap('x_data_vis.npy', dtype='float32', mode='r', shape=(n_lags*spect_height, num_data_samples_vis))\n",
    "ydata_vis = np.lib.format.open_memmap('y_data_vis.npy', dtype='float32', mode='r', shape=(spect_height, num_data_samples_vis))\n",
    "\n",
    "xdata.shape, ydata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set: memmap inputs\n",
    "data_vis = why_vis[:]\n",
    "x_data_vis = xdata_vis[:, :1000]\n",
    "y_data_vis = ydata_vis[:, :1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted for test set\n",
    "class Shuffled_memmap_testset(object):\n",
    "\n",
    "    def __init__(self, data, x_data, y_data, subset, n_lags, spect_height, vis_ratio=0, valid_ratio=.06):\n",
    "\n",
    "        # Set params\n",
    "        self.data = data\n",
    "        self.x_data = x_data\n",
    "        self.y_data = y_data\n",
    "        self.subset = subset\n",
    "        self.n_lags = n_lags\n",
    "        self.spect_height = spect_height\n",
    "        \n",
    "        self.data_idxs = np.arange(self.num_data_samples)\n",
    "        self.num_data_samples_vis = int(len(data) / spect_height)\n",
    "        self.data_idxs_vis = np.arange(self.num_data_samples_vis)\n",
    "        \n",
    "        np.random.shuffle(self.data_idxs)\n",
    "        self.valid_idxs, self.data_idxs = np.split(self.data_idxs, [self.num_valid_samples])\n",
    "        self.num_data_samples -= self.num_valid_samples\n",
    "\n",
    "        \n",
    "        def get_data(self, idx):\n",
    "            return self.x_data[:, idx].T, self.y_data[:, idx].T\n",
    "\n",
    "        def data_iterator(self, batch_size=64):\n",
    "            np.random.shuffle(self.data_idxs)\n",
    "            for batch_idx in range(0, self.num_data_samples, batch_size):\n",
    "                shuff_idx = self.data_idxs[batch_idx:batch_idx+batch_size]\n",
    "                yield self.get_data(shuff_idx)\n",
    "        \n",
    "        def vis_set(self):\n",
    "        #return self.get_data(self.vis_idxs)\n",
    "            return self.get_data(self.data_idxs_vis)\n",
    "\n",
    "        #def vis_iterator(self, start=0, end=None, batch_size=1):\n",
    "        #    if end is None:\n",
    "        #        end = self.num_vis_samples\n",
    "        #    for batch_idx in range(start, end, batch_size):\n",
    "        #        data_idx = self.vis_idxs[batch_idx:batch_idx+batch_size]\n",
    "        #        yield self.get_data(data_idx)\n",
    "\n",
    "        def vis_iterator(self, start=0, end=None, batch_size=1):\n",
    "            if end is None:\n",
    "                end = self.num_data_samples_vis\n",
    "            for batch_idx in range(start, end, batch_size):\n",
    "                data_idx = self.data_idxs_vis[batch_idx:batch_idx+batch_size]\n",
    "                yield self.get_data(data_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted for subset\n",
    "class Shuffled_memmap_subset(object):\n",
    "    \n",
    "    ''' For creating train/valid/test shuffled memmap dataset '''\n",
    "    \n",
    "    def __init__(self, data, x_data, y_data, subset, n_lags, spect_height, vis_ratio=0, valid_ratio=.06):\n",
    "        \n",
    "        # Set params\n",
    "        self.data = data\n",
    "        self.x_data = x_data\n",
    "        self.y_data = y_data\n",
    "        self.subset = subset\n",
    "        self.n_lags = n_lags\n",
    "        self.spect_height = spect_height\n",
    "        self.num_data_samples = int(len(data) / spect_height)\n",
    "        #self.num_data_samples\n",
    "\n",
    "        '''\n",
    "        # Initialize memmap handler\n",
    "        self.memmap_handler = MemmapHandler(self.folder, self.n_lags, self.spect_height, self.subset)\n",
    "\n",
    "        # Initialize data\n",
    "        data_loc = self.memmap_handler.read_file_loc(self.folder)\n",
    "        data = self.memmap_handler.init_data(self.folder, self.saved_data, x_location)\n",
    "        self.num_data_samples = self.memmap_handler.get_n_samples('train')\n",
    "        self.dataset = data.memmap_copy(data, data.shape[0], data.shape[1][:1000])\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        # get dataset+batching indicies\n",
    "        #self.num_vis_samples = int(self.num_data_samples*vis_ratio)\n",
    "        self.num_valid_samples = int(self.num_data_samples*valid_ratio)\n",
    "        #vis_start = np.random.randint(self.num_data_samples - self.num_vis_samples)\n",
    "       \n",
    "        #self.vis_idxs = np.arange(vis_start, vis_start + self.num_vis_samples)\n",
    "        #self.data_idxs = np.delete(np.arange(self.num_data_samples), self.vis_idxs)\n",
    "        self.data_idxs = np.arange(self.num_data_samples)\n",
    "        #self.num_data_samples -= self.num_vis_samples\n",
    "       \n",
    "        np.random.shuffle(self.data_idxs)\n",
    "        self.valid_idxs, self.data_idxs = np.split(self.data_idxs, [self.num_valid_samples])\n",
    "        self.num_data_samples -= self.num_valid_samples\n",
    "    \n",
    "    \n",
    "    def get_num_samples(self, subset):\n",
    "        ''' \n",
    "        Return number of samples wrt subset. \n",
    "            subset: a string - 'train', 'valid', or 'test'\n",
    "        '''\n",
    "        if subset == 'train':\n",
    "            return self.num_data_samples\n",
    "        elif subset == 'valid':\n",
    "            return self.num_valid_samples\n",
    "        elif subset == 'test':\n",
    "            return self.num_data_samples_vis\n",
    "    \n",
    "    def get_data(self, idx):\n",
    "        return self.x_data[:, idx].T, self.y_data[:, idx].T\n",
    "   \n",
    "    def data_iterator(self, batch_size=64):\n",
    "        np.random.shuffle(self.data_idxs)\n",
    "        for batch_idx in range(0, self.num_data_samples, batch_size):\n",
    "            shuff_idx = self.data_idxs[batch_idx:batch_idx+batch_size]\n",
    "            yield self.get_data(shuff_idx)\n",
    "   \n",
    "    def valid_set(self):\n",
    "        return self.get_data(self.valid_idxs)\n",
    "   \n",
    "    def valid_iterator(self, batch_size=64):\n",
    "        for batch_idx in range(0, self.num_valid_samples, batch_size):\n",
    "            valid_idx = self.valid_idxs[batch_idx:batch_idx+batch_size]\n",
    "            yield self.get_data(valid_idx)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Shuffled_memmap_subset(\n",
    "    data=data,\n",
    "    x_data=x_data,\n",
    "    y_data=y_data,\n",
    "    subset='train', \n",
    "    n_lags=2,\n",
    "    spect_height=32\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = Shuffled_memmap_subset(\n",
    "    data=data_vis,\n",
    "    x_data=x_data_vis,\n",
    "    y_data=y_data_vis,\n",
    "    subset='test', \n",
    "    n_lags=2,\n",
    "    spect_height=32\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the OG\n",
    "class Shuffled_memmap_dataset(object):\n",
    "    def __init__(self, xfile, yfile, xfile_vis, yfile_vis, vis_ratio=0, valid_ratio=.06):\n",
    "        y_data = np.memmap(yfile, dtype='float32', mode='r')\n",
    "        self.num_data_samples = int(len(y_data) / spect_height)\n",
    "       \n",
    "        y_data_vis = np.memmap(yfile_vis, dtype='float32', mode='r')\n",
    "        self.num_data_samples_vis = int(len(y_data_vis) / spect_height)\n",
    "       \n",
    "        self.x_data = np.memmap(xfile, dtype='float32', mode='r', shape=(n_lags*spect_height, self.num_data_samples))\n",
    "        self.y_data = np.memmap(yfile, dtype='float32', mode='r', shape=(spect_height, self.num_data_samples))\n",
    "       \n",
    "        self.x_data_vis = np.memmap(xfile_vis, dtype='float32', mode='r', shape=(n_lags*spect_height, self.num_data_samples_vis))\n",
    "        self.y_data_vis = np.memmap(yfile_vis, dtype='float32', mode='r', shape=(spect_height, self.num_data_samples_vis))\n",
    "               \n",
    "        #self.num_vis_samples = int(self.num_data_samples*vis_ratio)\n",
    "        self.num_valid_samples = int(self.num_data_samples*valid_ratio)\n",
    "        #vis_start = np.random.randint(self.num_data_samples - self.num_vis_samples)\n",
    "       \n",
    "        #self.vis_idxs = np.arange(vis_start, vis_start + self.num_vis_samples)\n",
    "        #self.data_idxs = np.delete(np.arange(self.num_data_samples), self.vis_idxs)\n",
    "        self.data_idxs_vis = np.arange(self.num_data_samples_vis)\n",
    "        self.data_idxs = np.arange(self.num_data_samples)\n",
    "        #self.num_data_samples -= self.num_vis_samples\n",
    "       \n",
    "        np.random.shuffle(self.data_idxs)\n",
    "        self.valid_idxs, self.data_idxs = np.split(self.data_idxs, [self.num_valid_samples])\n",
    "        self.num_data_samples -= self.num_valid_samples\n",
    "           \n",
    "    def get_data(self, idx):\n",
    "        return self.x_data[:, idx].T, self.y_data[:, idx].T\n",
    "   \n",
    "    def data_iterator(self, batch_size=64):\n",
    "        np.random.shuffle(self.data_idxs)\n",
    "        for batch_idx in range(0, self.num_data_samples, batch_size):\n",
    "            shuff_idx = self.data_idxs[batch_idx:batch_idx+batch_size]\n",
    "            yield self.get_data(shuff_idx)\n",
    "   \n",
    "    def valid_set(self):\n",
    "        return self.get_data(self.valid_idxs)\n",
    "   \n",
    "    def valid_iterator(self, batch_size=64):\n",
    "        for batch_idx in range(0, self.num_valid_samples, batch_size):\n",
    "            valid_idx = self.valid_idxs[batch_idx:batch_idx+batch_size]\n",
    "            yield self.get_data(valid_idx)\n",
    "   \n",
    "    def vis_set(self):\n",
    "        #return self.get_data(self.vis_idxs)\n",
    "        return self.get_data(self.data_idxs_vis)\n",
    "   \n",
    "    #def vis_iterator(self, start=0, end=None, batch_size=1):\n",
    "    #    if end is None:\n",
    "    #        end = self.num_vis_samples\n",
    "    #    for batch_idx in range(start, end, batch_size):\n",
    "    #        data_idx = self.vis_idxs[batch_idx:batch_idx+batch_size]\n",
    "    #        yield self.get_data(data_idx)\n",
    "           \n",
    "    def vis_iterator(self, start=0, end=None, batch_size=1):\n",
    "        if end is None:\n",
    "            end = self.num_data_samples_vis\n",
    "        for batch_idx in range(start, end, batch_size):\n",
    "            data_idx = self.data_idxs_vis[batch_idx:batch_idx+batch_size]\n",
    "            yield self.get_data(data_idx)\n",
    "\n",
    "#dataset = Shuffled_memmap_dataset(x_loc, y_loc, x_loc_vis, y_loc_vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "valid_data = Shuffled_memmap_subset(\n",
    "    folder='/home/AD/kachiem/memmap/memmap_dataset_stimulus/', \n",
    "    subset='valid', \n",
    "    saved_data,\n",
    "    n_lags=2,\n",
    "    spect_height=32\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_data = Shuffled_memmap_subset(\n",
    "    folder='/home/AD/kachiem/memmap/memmap_dataset_stimulus_vis/', \n",
    "    subset='train', \n",
    "    saved_data,\n",
    "    n_lags=2,\n",
    "    spect_height=32\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "#### Get visualization dataset\n",
    "vis_x, vis_y = dataset.vis_set()  #(16653, 64), (16653, 32)\n",
    "\n",
    "### took a subset of test set for visualization\n",
    "start = 3000\n",
    "end = start + 1000\n",
    "sub_vis_x = vis_x[start:end, :]   #(1000, 64)\n",
    "sub_vis_y = vis_y[start:end, :]   #(1000, 32) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapting MNIST classes to memmap iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doesn't really work right now\n",
    "class SortedMemmapGenerator(object):\n",
    "\n",
    "    ''' \n",
    "    Data generator providing lists of sorted memmap \n",
    "    '''\n",
    "\n",
    "    def __init__(batch_size, subset, terms, positive_samples=1, predict_terms=1, image_size=28, color=False, rescale=True):\n",
    "\n",
    "        # Set params\n",
    "        self.positive_samples = positive_samples\n",
    "        self.predict_terms = predict_terms\n",
    "        self.batch_size = batch_size\n",
    "        self.terms = terms\n",
    "        self.image_size = image_size\n",
    "        self.color = color\n",
    "        self.rescale = rescale\n",
    "\n",
    "        # Initialize memmap dataset\n",
    "        self.train_data = Shuffled_memmap_subset(\n",
    "            data=data,\n",
    "            x_data=x_data,\n",
    "            y_data=y_data,\n",
    "            subset='train', \n",
    "            n_lags=2,\n",
    "            spect_height=32\n",
    "            )\n",
    "\n",
    "        test_data = Shuffled_memmap_subset(\n",
    "            data=data_vis,\n",
    "            x_data=x_data_vis,\n",
    "            y_data=y_data_vis,\n",
    "            subset='test', \n",
    "            n_lags=2,\n",
    "            spect_height=32\n",
    "            )\n",
    "        \n",
    "        # get num_samples and num_batches\n",
    "        self.n_samples = dataset.get_num_samples(subset) \n",
    "        self.n_batches = self.n_samples // batch_size\n",
    "        \n",
    "        #self.n_samples = self.mnist_handler.get_n_samples(subset) // terms\n",
    "        #self.n_batches = self.n_samples // batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_batches\n",
    "\n",
    "    def next(self):\n",
    "\n",
    "        # Build sentences\n",
    "        image_labels = np.zeros((self.batch_size, self.terms + self.predict_terms))\n",
    "        sentence_labels = np.ones((self.batch_size, 1)).astype('int32')\n",
    "        positive_samples_n = self.positive_samples \n",
    "        for b in range(self.batch_size):\n",
    "\n",
    "            # Set ordered predictions for positive samples\n",
    "            seed = np.random.randint(0, 10)\n",
    "            sentence = np.mod(np.arange(seed, seed + self.terms + self.predict_terms), 10)\n",
    "\n",
    "            if positive_samples_n <= 0:\n",
    "\n",
    "                # Set random predictions for negative samples\n",
    "                # Each predicted term draws a number from a distribution that excludes itself\n",
    "                numbers = np.arange(0, 10)\n",
    "                predicted_terms = sentence[-self.predict_terms:]\n",
    "                for i, p in enumerate(predicted_terms):\n",
    "                    predicted_terms[i] = np.random.choice(numbers[numbers != p], 1)\n",
    "                sentence[-self.predict_terms:] = np.mod(predicted_terms, 10)\n",
    "                sentence_labels[b, :] = 0\n",
    "\n",
    "            # Save sentence\n",
    "            image_labels[b, :] = sentence\n",
    "\n",
    "            positive_samples_n -= 1\n",
    "\n",
    "        # Retrieve actual images (64, 64, 3) => (32, 2)\n",
    "        #images, _ = self.mnist_handler.get_batch_by_labels(self.subset, image_labels.flatten(), self.image_size, self.color, self.rescale)\n",
    "        images, _ = self.dataset.data_iterator(self.n_samples)\n",
    "        print(images)\n",
    "        \n",
    "        # Assemble batch\n",
    "        images = images.reshape((self.batch_size, self.terms + self.predict_terms, images.shape[1], images.shape[2]))\n",
    "        # (8, 8, 64, 64, 3)\n",
    "        x_images = images[:, :-self.predict_terms, ...]   # (8, 4, 64, 64, 3)\n",
    "        y_images = images[:, -self.predict_terms:, ...]   # (8, 4, 64, 64, 3)\n",
    "\n",
    "        # Randomize\n",
    "        idxs = np.random.choice(sentence_labels.shape[0], sentence_labels.shape[0], replace=False)\n",
    "        # shape: (8,)\n",
    "        \n",
    "        return [x_images[idxs, ...], y_images[idxs, ...]], sentence_labels[idxs, ...]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memmap ver\n",
    "def train_model(data, x_data, y_data, data_vis, x_data_vis, y_data_vis, n_lags, spect_height, epochs, batch_size, output_dir, code_size, lr=1e-4, terms=4, predict_terms=4, color=False):\n",
    "\n",
    "    # Prepare data\n",
    "    # clean up generator more to work by subset\n",
    "    train_data = Shuffled_memmap_subset(\n",
    "        data=data,\n",
    "        x_data=x_data,\n",
    "        y_data=y_data,\n",
    "        subset='train', \n",
    "        n_lags=n_lags,\n",
    "        spect_height=spect_height\n",
    "        )\n",
    "\n",
    "    test_data = Shuffled_memmap_subset(\n",
    "        data=data_vis,\n",
    "        x_data=x_data_vis,\n",
    "        y_data=y_data_vis,\n",
    "        subset='test', \n",
    "        n_lags=n_lags,\n",
    "        spect_height=spect_height\n",
    "        )\n",
    "\n",
    "    # Prepares the model\n",
    "    model = network_cpc(spect_height=spect_height, n_lags=n_lags, terms=terms, predict_terms=predict_terms,\n",
    "                        code_size=code_size, input_shape=(spect_height, n_lags), learning_rate=lr)\n",
    "\n",
    "    # Callbacks\n",
    "    history = keras.callbacks.History()\n",
    "    reduce_LR_Plateau = keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', \n",
    "        factor=1/3, \n",
    "        patience=2, \n",
    "        min_lr=1e-4)\n",
    "    #callbacks = [history, reduce_LR_Plateau]\n",
    "    callbacks = [history]\n",
    "    \n",
    "    # Trains the model\n",
    "    modeled = model.fit_generator(\n",
    "        generator=train_data,\n",
    "        steps_per_epoch=len(train_data),\n",
    "        validation_data=test_data,\n",
    "        validation_steps=len(test_data),\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    # Saves the model\n",
    "    # Remember to add custom_objects={'CPCLayer': CPCLayer} to load_model when loading from disk\n",
    "    model.save(join(output_dir, 'mem_cpc.h5'))\n",
    "\n",
    "    # Saves the encoder alone\n",
    "    encoder = model.layers[1].layer\n",
    "    encoder.save(join(output_dir, 'mem_encoder.h5'))\n",
    "    \n",
    "    # plotting loss\n",
    "    train_loss = modeled.history['loss']\n",
    "    val_loss = modeled.history['val_loss']\n",
    "    epoch_count = range(1, epochs+1)    # 10 epochs\n",
    "\n",
    "    plt.plot(epoch_count, train_loss, 'r--')\n",
    "    plt.plot(epoch_count, val_loss, 'o-')\n",
    "    plt.legend(['Training Loss', 'Validation Loss'])\n",
    "    plt.title('')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    t = time.localtime()\n",
    "    timestamp = time.strftime('%b-%d-%Y_%H%M', t)\n",
    "    plt.savefig(output_dir + \"img/\" + timestamp + \"_mem.png\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_encoder(x, code_size):\n",
    "\n",
    "    ''' Define the network mapping images to embeddings '''\n",
    "\n",
    "    x = keras.layers.Conv2D(filters=10, kernel_size=3, strides=5, data_format=\"channels_first\", activation='relu', input_shape=(32, 2))(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.LeakyReLU()(x)\n",
    "    x = keras.layers.Conv2D(filters=8, kernel_size=3, strides=4, activation='relu')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.LeakyReLU()(x)\n",
    "    x = keras.layers.Conv2D(filters=4, kernel_size=3, strides=2, activation='relu')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.LeakyReLU()(x)\n",
    "    x = keras.layers.Conv2D(filters=4, kernel_size=3, strides=2, activation='relu')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.LeakyReLU()(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Dense(units=512, activation='relu')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.LeakyReLU()(x)\n",
    "    x = keras.layers.Dense(units=code_size, activation='relu', name='encoder_embedding')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def network_autoregressive(x):\n",
    "\n",
    "    ''' Define the network that integrates information along the sequence '''\n",
    "\n",
    "    # x = keras.layers.GRU(units=256, return_sequences=True)(x)\n",
    "    # x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.GRU(units=256, return_sequences=False, name='ar_context')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def network_prediction(context, code_size, predict_terms):\n",
    "\n",
    "    ''' Define the network mapping context to multiple embeddings '''\n",
    "\n",
    "    outputs = []\n",
    "    for i in range(predict_terms):\n",
    "        outputs.append(keras.layers.Dense(units=code_size, activation=\"linear\", name='z_t_{i}'.format(i=i))(context))\n",
    "\n",
    "    if len(outputs) == 1:\n",
    "        output = keras.layers.Lambda(lambda x: K.expand_dims(x, axis=1))(outputs[0])\n",
    "    else:\n",
    "        output = keras.layers.Lambda(lambda x: K.stack(x, axis=1))(outputs)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def network_cpc(spect_height, n_lags, terms, predict_terms, code_size, input_shape, learning_rate):\n",
    "\n",
    "    ''' Define the CPC network combining encoder and autoregressive model '''\n",
    "\n",
    "    # Set learning phase (https://stackoverflow.com/questions/42969779/keras-error-you-must-feed-a-value-for-placeholder-tensor-bidirectional-1-keras)\n",
    "    K.set_learning_phase(1)\n",
    "\n",
    "    # Define encoder model\n",
    "    encoder_input = keras.layers.Input(shape=input_shape)\n",
    "    encoder_output = network_encoder(encoder_input, code_size)\n",
    "    encoder_model = keras.models.Model(encoder_input, encoder_output, name='encoder')\n",
    "    encoder_model.summary()\n",
    "\n",
    "    # Define rest of model\n",
    "    x_input = keras.layers.Input((terms, input_shape))\n",
    "    x_encoded = keras.layers.TimeDistributed(encoder_model)(x_input)\n",
    "    context = network_autoregressive(x_encoded)\n",
    "    preds = network_prediction(context, code_size, predict_terms)\n",
    "\n",
    "    y_input = keras.layers.Input((predict_terms, input_shape))\n",
    "    y_encoded = keras.layers.TimeDistributed(encoder_model)(y_input)\n",
    "\n",
    "    # Loss\n",
    "    dot_product_probs = CPCLayer()([preds, y_encoded])\n",
    "\n",
    "    # Model\n",
    "    cpc_model = keras.models.Model(inputs=[x_input, y_input], outputs=dot_product_probs)\n",
    "\n",
    "    # Compile model\n",
    "    cpc_model.compile(\n",
    "        optimizer=keras.optimizers.Adam(lr=learning_rate),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['binary_accuracy']\n",
    "    )\n",
    "    cpc_model.summary()\n",
    "\n",
    "    return cpc_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer conv2d_4 is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: [None, 32, 2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-7d4ec248c534>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mterms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mpredict_terms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-44d6f90b0c69>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(data, x_data, y_data, data_vis, x_data_vis, y_data_vis, n_lags, spect_height, epochs, batch_size, output_dir, code_size, lr, terms, predict_terms, color)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Prepares the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     model = network_cpc(spect_height=spect_height, n_lags=n_lags, terms=terms, predict_terms=predict_terms,\n\u001b[0;32m---> 26\u001b[0;31m                         code_size=code_size, input_shape=(spect_height, n_lags), learning_rate=lr)\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-73917e713628>\u001b[0m in \u001b[0;36mnetwork_cpc\u001b[0;34m(spect_height, n_lags, terms, predict_terms, code_size, input_shape, learning_rate)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# Define encoder model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mencoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mencoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0mencoder_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'encoder'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mencoder_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-73917e713628>\u001b[0m in \u001b[0;36mnetwork_encoder\u001b[0;34m(x, code_size)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m''' Define the network mapping images to embeddings '''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"channels_first\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLeakyReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;31m# Build layer if applicable (if the `build` method has been overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m         \u001b[0;31m# We must set self.built since user defined build functions are not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;31m# constrained to set self.built.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m     \u001b[0;31m# Check input assumptions set before layer building, e.g. input rank.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m     input_spec.assert_input_compatibility(\n\u001b[0;32m-> 1591\u001b[0;31m         self.input_spec, inputs, self.name)\n\u001b[0m\u001b[1;32m   1592\u001b[0m     \u001b[0minput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput_list\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    121\u001b[0m                          \u001b[0;34m'expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. Full shape received: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                          str(x.shape.as_list()))\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m       \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer conv2d_4 is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: [None, 32, 2]"
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    # x/y train data\n",
    "    data=data,\n",
    "    x_data=x_data,\n",
    "    y_data=y_data,\n",
    "    # x/y test data\n",
    "    data_vis=data_vis,\n",
    "    x_data_vis=x_data_vis,\n",
    "    y_data_vis=y_data_vis,\n",
    "    \n",
    "    # params\n",
    "    n_lags=2, \n",
    "    spect_height=32,\n",
    "    epochs=10,\n",
    "    batch_size=8,\n",
    "    output_dir='models/memmap',\n",
    "    code_size=128,\n",
    "    lr=2e-4,\n",
    "    terms=3,\n",
    "    predict_terms=3,\n",
    "    color=True\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss fxn\n",
    "def train_model(x_data, y_data, x_loc_vis, y_loc_vis, epochs, batch_size, output_dir, code_size, lr=1e-4, terms=4, predict_terms=4, image_size=28, color=False):\n",
    "\n",
    "    # Prepare data\n",
    "    # clean up generator more to work by subset\n",
    "    train_data = SortedMemmapGenerator(x_data, y_data, x_loc_vis, y_loc_vis, batch_size=batch_size, subset='train', terms=terms,\n",
    "                                       positive_samples=batch_size // 2, predict_terms=predict_terms,\n",
    "                                       image_size=image_size, color=color, rescale=True)\n",
    "\n",
    "    validation_data = SortedMemmapGenerator(x_data, y_data, x_loc_vis, y_loc_vis, batch_size=batch_size, subset='valid', terms=terms,\n",
    "                                            positive_samples=batch_size // 2, predict_terms=predict_terms,\n",
    "                                            image_size=image_size, color=color, rescale=True)\n",
    "\n",
    "    \n",
    "    # Prepares the model\n",
    "    model = network_cpc(image_shape=(image_size, image_size, 3), terms=terms, predict_terms=predict_terms,\n",
    "                        code_size=code_size, learning_rate=lr)\n",
    "\n",
    "    # Callbacks\n",
    "    history = keras.callbacks.History()\n",
    "    reduce_LR_Plateau = keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', \n",
    "        factor=1/3, \n",
    "        patience=2, \n",
    "        min_lr=1e-4)\n",
    "    #callbacks = [history, reduce_LR_Plateau]\n",
    "    callbacks = [history]\n",
    "    \n",
    "    # Trains the model\n",
    "    modeled = model.fit_generator(\n",
    "        generator=train_data,\n",
    "        steps_per_epoch=len(train_data),\n",
    "        validation_data=validation_data,\n",
    "        validation_steps=len(validation_data),\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    # Saves the model\n",
    "    # Remember to add custom_objects={'CPCLayer': CPCLayer} to load_model when loading from disk\n",
    "    model.save(join(output_dir, 'mem_cpc.h5'))\n",
    "\n",
    "    # Saves the encoder alone\n",
    "    encoder = model.layers[1].layer\n",
    "    encoder.save(join(output_dir, 'mem_encoder.h5'))\n",
    "    \n",
    "    # plotting loss\n",
    "    train_loss = modeled.history['loss']\n",
    "    val_loss = modeled.history['val_loss']\n",
    "    epoch_count = range(1, epochs+1)    # 10 epochs\n",
    "\n",
    "    plt.plot(epoch_count, train_loss, 'r--')\n",
    "    plt.plot(epoch_count, val_loss, 'o-')\n",
    "    plt.legend(['Training Loss', 'Validation Loss'])\n",
    "    plt.title('')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    t = time.localtime()\n",
    "    timestamp = time.strftime('%b-%d-%Y_%H%M', t)\n",
    "    plt.savefig(output_dir + \"img/\" + timestamp + \"_mem.png\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main( ) in train_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cpc train_model() params / main()\n",
    "SPECT_HEIGHT = 2048\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "output_dir = \"main\"\n",
    "code_size = 8\n",
    "hparams = lspct.features.spectrogram.HPARAMS\n",
    "\n",
    "#path_list = glob.glob((lspct.paths.WAV_DIR / \"*.wav\").as_posix())[0:19]    # get list of wavs\n",
    "#training_path_list, validation_path_list = split_validation(path_list, 0.1) # split train and val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(\n",
    "    x_data, y_data, x_loc_vis, y_loc_vis,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    output_dir='models/memmap',\n",
    "    code_size=128,\n",
    "    lr=1e-3,\n",
    "    terms=4,\n",
    "    predict_terms=4,\n",
    "    image_size=64,\n",
    "    color=True\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
