{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/AD/kachiem/miniconda3/envs/tf-conda/bin/python3\n",
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob as glob\n",
    "from os.path import join, basename, dirname, exists\n",
    "import sys\n",
    "print(sys.executable)  #print kernel path\n",
    "#print(sys.path)\n",
    "\n",
    "# tensorflow imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "print(tf.__version__)\n",
    "\n",
    "# custom imports\n",
    "import librispect as lspct\n",
    "from librispect.features import predict\n",
    "from librispect.utils import split_validation\n",
    "\n",
    "from data_utils import SortedNumberGenerator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attempting to connect srihita's spectrogram function to train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_encoder(x, code_size):\n",
    "\n",
    "    ''' Define the network mapping images to embeddings '''\n",
    "\n",
    "    x = keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, activation='linear')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.LeakyReLU()(x)\n",
    "    x = keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, activation='linear')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.LeakyReLU()(x)\n",
    "    x = keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, activation='linear')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.LeakyReLU()(x)\n",
    "    x = keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, activation='linear')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.LeakyReLU()(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Dense(units=256, activation='linear')(x)\n",
    "    x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.LeakyReLU()(x)\n",
    "    x = keras.layers.Dense(units=code_size, activation='linear', name='encoder_embedding')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def network_autoregressive(x):\n",
    "\n",
    "    ''' Define the network that integrates information along the sequence '''\n",
    "\n",
    "    # x = keras.layers.GRU(units=256, return_sequences=True)(x)\n",
    "    # x = keras.layers.BatchNormalization()(x)\n",
    "    x = keras.layers.GRU(units=256, return_sequences=False, name='ar_context')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def network_prediction(context, code_size, predict_terms):\n",
    "\n",
    "    ''' Define the network mapping context to multiple embeddings '''\n",
    "\n",
    "    outputs = []\n",
    "    for i in range(predict_terms):\n",
    "        outputs.append(keras.layers.Dense(units=code_size, activation=\"linear\", name='z_t_{i}'.format(i=i))(context))\n",
    "\n",
    "    if len(outputs) == 1:\n",
    "        output = keras.layers.Lambda(lambda x: K.expand_dims(x, axis=1))(outputs[0])\n",
    "    else:\n",
    "        output = keras.layers.Lambda(lambda x: K.stack(x, axis=1))(outputs)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "class CPCLayer(keras.layers.Layer):\n",
    "\n",
    "    ''' Computes dot product between true and predicted embedding vectors '''\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CPCLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        # Compute dot product among vectors\n",
    "        preds, y_encoded = inputs\n",
    "        dot_product = K.mean(y_encoded * preds, axis=-1)\n",
    "        dot_product = K.mean(dot_product, axis=-1, keepdims=True)  # average along the temporal dimension\n",
    "\n",
    "        # Keras loss functions take probabilities\n",
    "        dot_product_probs = K.sigmoid(dot_product)\n",
    "\n",
    "        return dot_product_probs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], 1)\n",
    "\n",
    "\n",
    "def network_cpc(image_shape, terms, predict_terms, code_size, learning_rate):\n",
    "\n",
    "    ''' Define the CPC network combining encoder and autoregressive model '''\n",
    "\n",
    "    # Set learning phase (https://stackoverflow.com/questions/42969779/keras-error-you-must-feed-a-value-for-placeholder-tensor-bidirectional-1-keras)\n",
    "    K.set_learning_phase(1)\n",
    "\n",
    "    # Define encoder model\n",
    "    encoder_input = keras.layers.Input(image_shape)\n",
    "    encoder_output = network_encoder(encoder_input, code_size)\n",
    "    encoder_model = keras.models.Model(encoder_input, encoder_output, name='encoder')\n",
    "    encoder_model.summary()\n",
    "\n",
    "    # Define rest of model\n",
    "    x_input = keras.layers.Input((terms, image_shape[0], image_shape[1], image_shape[2]))\n",
    "    x_encoded = keras.layers.TimeDistributed(encoder_model)(x_input)\n",
    "    context = network_autoregressive(x_encoded)\n",
    "    preds = network_prediction(context, code_size, predict_terms)\n",
    "\n",
    "    y_input = keras.layers.Input((predict_terms, image_shape[0], image_shape[1], image_shape[2]))\n",
    "    y_encoded = keras.layers.TimeDistributed(encoder_model)(y_input)\n",
    "\n",
    "    # Loss\n",
    "    dot_product_probs = CPCLayer()([preds, y_encoded])\n",
    "\n",
    "    # Model\n",
    "    cpc_model = keras.models.Model(inputs=[x_input, y_input], outputs=dot_product_probs)\n",
    "\n",
    "    # Compile model\n",
    "    cpc_model.compile(\n",
    "        optimizer=keras.optimizers.Adam(lr=learning_rate),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['binary_accuracy']\n",
    "    )\n",
    "    cpc_model.summary()\n",
    "\n",
    "    return cpc_model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs, batch_size, output_dir, code_size, lr=1e-4, terms=4, predict_terms=4, image_size=28, color=False):\n",
    "\n",
    "    # Prepare data\n",
    "    train_data = SortedNumberGenerator(batch_size=batch_size, subset='train', terms=terms,\n",
    "                                       positive_samples=batch_size // 2, predict_terms=predict_terms,\n",
    "                                       image_size=image_size, color=color, rescale=True)\n",
    "\n",
    "    validation_data = SortedNumberGenerator(batch_size=batch_size, subset='valid', terms=terms,\n",
    "                                            positive_samples=batch_size // 2, predict_terms=predict_terms,\n",
    "                                            image_size=image_size, color=color, rescale=True)\n",
    "\n",
    "    # Prepares the model\n",
    "    model = network_cpc(image_shape=(image_size, image_size, 3), terms=terms, predict_terms=predict_terms,\n",
    "                        code_size=code_size, learning_rate=lr)\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = [keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=1/3, patience=2, min_lr=1e-4)]\n",
    "\n",
    "    # Trains the model\n",
    "    model.fit_generator(\n",
    "        generator=train_data,\n",
    "        steps_per_epoch=len(train_data),\n",
    "        validation_data=validation_data,\n",
    "        validation_steps=len(validation_data),\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    # Saves the model\n",
    "    # Remember to add custom_objects={'CPCLayer': CPCLayer} to load_model when loading from disk\n",
    "    model.save(join(output_dir, 'cpc.h5'))\n",
    "\n",
    "    # Saves the encoder alone\n",
    "    encoder = model.layers[1].layer\n",
    "    encoder.save(join(output_dir, 'encoder.h5'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shaping input for spectrogram functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Instead of images and labels, I have sectrogram segments x and y.\n",
    "So x is input and y would be output (aka labels). \n",
    "You train on bunch of (x,y), let's call it train_x and train_y.\n",
    "Then when you input test_x, you should get test_y (or vis_x and vis_y in this case)\n",
    "'''\n",
    "\n",
    "# spect_height = number of freq. bins\n",
    "# y always has one time bin\n",
    "# n_lags is number of time bins of x\n",
    "# shape of x = (n_lags,spect_height) x number of samples\n",
    "# shape of y = (1, spect_height) x number of samples\n",
    "\n",
    "n_lags = 2\n",
    "spect_height = 32\n",
    "NIN = n_lags*spect_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location\n",
    "datafolder = '/home/AD/kachiem/memmap/memmap_dataset_stimulus/'\n",
    "visfolder = '/home/AD/kachiem/memmap/memmap_dataset_stimulus_vis/'\n",
    "x_loc = '%sx_lag%03d.dat' % (datafolder, n_lags)\n",
    "y_loc = '%sy_lag%03d.dat' % (datafolder, n_lags)\n",
    "x_loc_vis = '%sx_lag%03d.dat' % (visfolder, n_lags)\n",
    "y_loc_vis = '%sy_lag%03d.dat' % (visfolder, n_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((64, 9597568), (32, 9597568))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get train data\n",
    "y = np.memmap(x_loc, dtype='float32', mode='r')\n",
    "num_data_samples = int(len(y) / spect_height)\n",
    "x_data = np.memmap(x_loc, dtype='float32', mode='r+', shape=(n_lags*spect_height, num_data_samples))\n",
    "y_data = np.memmap(y_loc, dtype='float32', mode='r+', shape=(spect_height, num_data_samples))\n",
    "\n",
    "x_data.shape, y_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "srihita's spectrogram functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Shuffled_memmap_dataset:\n",
    "    def __init__(self, xfile, yfile, xfile_vis, yfile_vis, vis_ratio=0, valid_ratio=.06):\n",
    "        y_data = np.memmap(yfile, dtype='float32', mode='r')\n",
    "        self.num_data_samples = int(len(y_data) / spect_height)\n",
    "       \n",
    "        y_data_vis = np.memmap(yfile_vis, dtype='float32', mode='r')\n",
    "        self.num_data_samples_vis = int(len(y_data_vis) / spect_height)\n",
    "       \n",
    "        self.x_data = np.memmap(xfile, dtype='float32', mode='r', shape=(n_lags*spect_height, self.num_data_samples))\n",
    "        self.y_data = np.memmap(yfile, dtype='float32', mode='r', shape=(spect_height, self.num_data_samples))\n",
    "       \n",
    "        self.x_data_vis = np.memmap(xfile_vis, dtype='float32', mode='r', shape=(n_lags*spect_height, self.num_data_samples_vis))\n",
    "        self.y_data_vis = np.memmap(yfile_vis, dtype='float32', mode='r', shape=(spect_height, self.num_data_samples_vis))\n",
    "               \n",
    "        #self.num_vis_samples = int(self.num_data_samples*vis_ratio)\n",
    "        self.num_valid_samples = int(self.num_data_samples*valid_ratio)\n",
    "        #vis_start = np.random.randint(self.num_data_samples - self.num_vis_samples)\n",
    "       \n",
    "        #self.vis_idxs = np.arange(vis_start, vis_start + self.num_vis_samples)\n",
    "        #self.data_idxs = np.delete(np.arange(self.num_data_samples), self.vis_idxs)\n",
    "        self.data_idxs_vis = np.arange(self.num_data_samples_vis)\n",
    "        self.data_idxs = np.arange(self.num_data_samples)\n",
    "        #self.num_data_samples -= self.num_vis_samples\n",
    "       \n",
    "        np.random.shuffle(self.data_idxs)\n",
    "        self.valid_idxs, self.data_idxs = np.split(self.data_idxs, [self.num_valid_samples])\n",
    "        self.num_data_samples -= self.num_valid_samples\n",
    "           \n",
    "    def get_data(self, idx):\n",
    "        return self.x_data[:, idx].T, self.y_data[:, idx].T\n",
    "   \n",
    "    def data_iterator(self, batch_size=64):\n",
    "        np.random.shuffle(self.data_idxs)\n",
    "        for batch_idx in range(0, self.num_data_samples, batch_size):\n",
    "            shuff_idx = self.data_idxs[batch_idx:batch_idx+batch_size]\n",
    "            yield self.get_data(shuff_idx)\n",
    "   \n",
    "    def valid_set(self):\n",
    "        return self.get_data(self.valid_idxs)\n",
    "   \n",
    "    def valid_iterator(self, batch_size=64):\n",
    "        for batch_idx in range(0, self.num_valid_samples, batch_size):\n",
    "            valid_idx = self.valid_idxs[batch_idx:batch_idx+batch_size]\n",
    "            yield self.get_data(valid_idx)\n",
    "   \n",
    "    def vis_set(self):\n",
    "        #return self.get_data(self.vis_idxs)\n",
    "        return self.get_data(self.data_idxs_vis)\n",
    "   \n",
    "    #def vis_iterator(self, start=0, end=None, batch_size=1):\n",
    "    #    if end is None:\n",
    "    #        end = self.num_vis_samples\n",
    "    #    for batch_idx in range(start, end, batch_size):\n",
    "    #        data_idx = self.vis_idxs[batch_idx:batch_idx+batch_size]\n",
    "    #        yield self.get_data(data_idx)\n",
    "           \n",
    "    def vis_iterator(self, start=0, end=None, batch_size=1):\n",
    "        if end is None:\n",
    "            end = self.num_data_samples_vis\n",
    "        for batch_idx in range(start, end, batch_size):\n",
    "            data_idx = self.data_idxs_vis[batch_idx:batch_idx+batch_size]\n",
    "            yield self.get_data(data_idx)\n",
    "\n",
    "dataset = Shuffled_memmap_dataset(x_loc, y_loc, x_loc_vis, y_loc_vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "putting it all together - input + spectrogram + train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get visualization dataset\n",
    "vis_x, vis_y = dataset.vis_set()\n",
    "# took a subset of test set for visualization\n",
    "start = 3000\n",
    "end = start + 1000\n",
    "sub_vis_x = vis_x[start:end, :]\n",
    "sub_vis_y = vis_y[start:end, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main( ) in train_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cpc train_model() params / main()\n",
    "SPECT_HEIGHT = 2048\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "output_dir = \"main\"\n",
    "code_size = 8\n",
    "hparams = lspct.features.spectrogram.HPARAMS\n",
    "\n",
    "#path_list = glob.glob((lspct.paths.WAV_DIR / \"*.wav\").as_posix())[0:19]    # get list of wavs\n",
    "#training_path_list, validation_path_list = split_validation(path_list, 0.1) # split train and val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/AD/kachiem/miniconda3/envs/tf-conda/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 64, 64, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 31, 31, 64)        1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 31, 31, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 31, 31, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 15, 15, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 15, 15, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 7, 7, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 3, 3, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               147712    \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "encoder_embedding (Dense)    (None, 128)               32896     \n",
      "=================================================================\n",
      "Total params: 295,232\n",
      "Trainable params: 294,208\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 4, 64, 64, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 4, 128)       295232      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "ar_context (GRU)                (None, 256)          295680      time_distributed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "z_t_0 (Dense)                   (None, 128)          32896       ar_context[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "z_t_1 (Dense)                   (None, 128)          32896       ar_context[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "z_t_2 (Dense)                   (None, 128)          32896       ar_context[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "z_t_3 (Dense)                   (None, 128)          32896       ar_context[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 4, 64, 64, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 4, 128)       0           z_t_0[0][0]                      \n",
      "                                                                 z_t_1[0][0]                      \n",
      "                                                                 z_t_2[0][0]                      \n",
      "                                                                 z_t_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 4, 128)       295232      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cpc_layer (CPCLayer)            (None, 1)            0           lambda[0][0]                     \n",
      "                                                                 time_distributed_1[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 722,496\n",
      "Trainable params: 721,472\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SortedNumberGenerator' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-626128df23d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpredict_terms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-2-27af98d28f73>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(epochs, batch_size, output_dir, code_size, lr, terms, predict_terms, image_size, color)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     )\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1424\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/miniconda3/envs/tf-conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m       \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m       \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m       shuffle=shuffle)\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m   \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf-conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mconvert_to_generator_like\u001b[0;34m(data, batch_size, steps_per_epoch, epochs, shuffle)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m   \u001b[0;31m# Create generator from NumPy or EagerTensor Input.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m   \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'You must specify `batch_size`'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SortedNumberGenerator' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    output_dir='models/memmap',\n",
    "    code_size=128,\n",
    "    lr=1e-3,\n",
    "    terms=4,\n",
    "    predict_terms=4,\n",
    "    image_size=64,\n",
    "    color=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
